{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-step-task as described in \"Prefrontal cortex as a meta-reinforcement learning system\"\n",
    "\n",
    "This iPython notebook includes an implementation of the two-step task as described here: [biorxiv link](https://www.biorxiv.org/content/early/2018/04/13/295964).\n",
    "\n",
    "The difference with our first trial (in `biorxiv-first-try.ipynb`) is that, in this notebook, **the A2C LSTM receives only actions made at stage 1 and the rewards received after leaving stage 2.** Therefore, the LSTM receives \"two-steps in one\", as implemented in \"trial\" from the class \"two_step_task()\".\n",
    "\n",
    "Note: in the biorxiv paper, they do not point out to any state in the first_stage, and refer to the second stage states as S_1 and S_2. In this notebook, we use the terminology presented in their [previous work on arxiv](https://arxiv.org/abs/1611.05763), where at the first stage there is one state S_1, and at the second stage there are two states S_2 and S_3.\n",
    "\n",
    "For this final step, the goal was to reproduce the plots from the [biorxiv pre-print](https://www.biorxiv.org/content/early/2018/04/13/295964) (Simulation 4, Figure b) ). To that end, we launched n=8 trainings using different seeds, but with the same hyperparameters as the paper, to compare to the results obtained by Wang et al.\n",
    "\n",
    "For each seed, the training consisted of 20k episodes of 100 trials (instead of 10k episodes of 100 trials in the paper). The reason for our number of episodes choice is that, in our case, the learning seemed to converge after around ~20k episodes for most seeds, without any significant gap in reward before ~15k episodes.\n",
    "\n",
    " ![reward curve](results/biorxiv/final/reward_curve.png)\n",
    " \n",
    "After training, we tested the 8 different models for 300 further episodes (like in the paper), with the weights of the LSTM being fixed. \n",
    "\n",
    "Here is the side by side comparison of our results (on the left) with the results from the paper (on the right):\n",
    "\n",
    "![side by side](results/biorxiv/final/side_by_side.png)\n",
    "\n",
    "Running the cells below will reproduce those tests. It will generate 8 different plots of the probabilities of repeating an action for a common/uncommon transition, if the last action was rewarded/unrewarded. Finally, it will average those plots to output a final plot, to reproduce the Figure b) from Simulation 4 in biorxiv. Each datapoint from a different seed is represented by a black dot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marvin/opt/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from helper import *\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_slim as slim\n",
    "import scipy.signal\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw \n",
    "from PIL import ImageFont\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories/book-keeping\n",
    "from datetime import datetime\n",
    "\n",
    "dir_name = \"train_\" + datetime.now().strftime(\"%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding of the higher stages\n",
    "S_1 = 0\n",
    "S_2 = 1\n",
    "S_3 = 2\n",
    "nb_states = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of our environment: the two-step task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_step_task():\n",
    "    def __init__(self):\n",
    "        # start in S_1\n",
    "        self.state = S_1\n",
    "        \n",
    "        # defines what is the stage with the highest expected reward. Initially random\n",
    "        self.highest_reward_second_stage = np.random.choice([S_2,S_3])\n",
    "        \n",
    "        self.num_actions = 2\n",
    "        self.reset()\n",
    "        \n",
    "        # initialization of plotting variables\n",
    "        common_prob = 0.8\n",
    "        self.transitions = np.array([\n",
    "            [common_prob, 1-common_prob],\n",
    "            [1-common_prob, common_prob]\n",
    "        ])\n",
    "        self.transition_count = np.zeros((2,2,2))\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "    \n",
    "    def get_state(self):\n",
    "        one_hot_array = np.zeros(nb_states)\n",
    "        one_hot_array[self.state] = 1\n",
    "        return one_hot_array\n",
    "\n",
    "    def possible_switch(self):\n",
    "        if (np.random.uniform() < 0.025):\n",
    "            # switches which of S_2 or S_3 has expected reward of 0.9\n",
    "            self.highest_reward_second_stage = S_2 if (self.highest_reward_second_stage == S_3) else S_3\n",
    "            \n",
    "    def get_rprobs(self):\n",
    "        \"\"\"\n",
    "        probability of reward of states S_2 and S_3, in the form [[p, 1-p], [1-p, p]]\n",
    "        \"\"\"\n",
    "        if (self.highest_reward_second_stage == S_2):\n",
    "            r_prob = 0.9\n",
    "        else:\n",
    "            r_prob = 0.1\n",
    "        \n",
    "        rewards = np.array([\n",
    "            [r_prob, 1-r_prob],\n",
    "            [1-r_prob, r_prob]\n",
    "        ])\n",
    "        return rewards\n",
    "            \n",
    "    def isCommon(self,action,state):\n",
    "        if self.transitions[action][state] >= 1/2:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def updateStateProb(self,action):\n",
    "        if self.last_is_rewarded: #R\n",
    "            if self.last_is_common: #C\n",
    "                if self.last_action == action: #Rep\n",
    "                    self.transition_count[0,0,0] += 1\n",
    "                else: #URep\n",
    "                    self.transition_count[0,0,1] += 1\n",
    "            else: #UC\n",
    "                if self.last_action == action: #Rep\n",
    "                    self.transition_count[0,1,0] += 1\n",
    "                else: #URep\n",
    "                    self.transition_count[0,1,1] += 1\n",
    "        else: #UR\n",
    "            if self.last_is_common:\n",
    "                if self.last_action == action:\n",
    "                    self.transition_count[1,0,0] += 1\n",
    "                else:\n",
    "                    self.transition_count[1,0,1] += 1\n",
    "            else:\n",
    "                if self.last_action == action:\n",
    "                    self.transition_count[1,1,0] += 1\n",
    "                else:\n",
    "                    self.transition_count[1,1,1] += 1\n",
    "                    \n",
    "        \n",
    "    def stayProb(self):\n",
    "        print(self.transition_count)\n",
    "        row_sums = self.transition_count.sum(axis=-1)\n",
    "        stay_prob = self.transition_count / row_sums[:,:,np.newaxis] \n",
    "       \n",
    "        return stay_prob\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        \n",
    "        # for the two-step task plots\n",
    "        self.last_is_common = None\n",
    "        self.last_is_rewarded = None\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        \n",
    "        # come back to S_1 at the end of an episode\n",
    "        self.state = S_1\n",
    "        \n",
    "        return self.get_state()\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.timestep += 1\n",
    "        self.last_state = self.state\n",
    "        \n",
    "        # get next stage\n",
    "        if (self.state == S_1):\n",
    "            # get reward\n",
    "            reward = 0\n",
    "            # update stage\n",
    "            self.state = S_2 if (np.random.uniform() < self.transitions[action][0]) else S_3\n",
    "            # keep track of stay probability after first action\n",
    "            if (self.last_action != None):    \n",
    "                self.updateStateProb(action)\n",
    "            self.last_action = action\n",
    "            # book-keeping for plotting\n",
    "            self.last_is_common = self.isCommon(action,self.state-1)\n",
    "            \n",
    "        else:# case S_2 or S_3\n",
    "            # get probability of reward in stage\n",
    "            r_prob = 0.9 if (self.highest_reward_second_stage == self.state) else 0.1\n",
    "            # get reward\n",
    "            reward = 1 if np.random.uniform() < r_prob else 0\n",
    "            # update stage\n",
    "            self.state = S_1\n",
    "            # book-keeping for plotting\n",
    "            self.last_is_rewarded = reward\n",
    "\n",
    "        # new state after the decision\n",
    "        new_state = self.get_state()\n",
    "        if self.timestep >= 200: \n",
    "            done = True\n",
    "        else: \n",
    "            done = False\n",
    "        return new_state,reward,done,self.timestep\n",
    "    \n",
    "    def trial(self,action):\n",
    "        # do one action in S_1, and keep track of the perceptually distinguishable state you arive in\n",
    "        observation,_,_,_ = self.step(action)\n",
    "        # do the same action in the resulting state (S_2 or S_3). The action doesn't matter, the reward does\n",
    "        _,reward,done,_ = self.step(action)\n",
    "        return observation,reward,done,self.timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,a_size,scope,trainer):\n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.state = tf.compat.v1.placeholder(shape=[None,3],dtype=tf.float32)\n",
    "            self.prev_rewards = tf.compat.v1.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "            self.prev_actions = tf.compat.v1.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.timestep = tf.compat.v1.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "            self.prev_actions_onehot = tf.one_hot(self.prev_actions,a_size,dtype=tf.float32)\n",
    "\n",
    "            hidden = tf.concat([slim.flatten(self.state),self.prev_rewards,self.prev_actions_onehot,self.timestep],1)\n",
    "\n",
    "            print(\"HIDDEN\")\n",
    "            print(hidden)\n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(48,state_is_tuple=True)\n",
    "\n",
    "            print(\"lstm_cell\")\n",
    "            print(lstm_cell)\n",
    "\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "\n",
    "            print(\"state_init\")\n",
    "            print(self.state_init)\n",
    "\n",
    "            c_in = tf.compat.v1.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.compat.v1.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "\n",
    "            print(\"state_in\")\n",
    "            print(self.state_in)\n",
    "\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(input=self.prev_rewards)[:1]\n",
    "            state_in = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.compat.v1.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "\n",
    "            print(\"state_out\")\n",
    "            print(self.state_out)\n",
    "\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 48])\n",
    "\n",
    "            print(\"rnn_out\")\n",
    "            print(rnn_out)\n",
    "            \n",
    "            self.actions = tf.compat.v1.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                        \n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                weights_initializer=normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.target_v = tf.compat.v1.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.compat.v1.placeholder(shape=[None],dtype=tf.float32)\n",
    "                \n",
    "                self.responsible_outputs = tf.reduce_sum(input_tensor=self.policy * self.actions_onehot, axis=[1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(input_tensor=tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(input_tensor=self.policy * tf.math.log(self.policy + 1e-7))\n",
    "                self.policy_loss = -tf.reduce_sum(input_tensor=tf.math.log(self.responsible_outputs + 1e-7)*self.advantages)\n",
    "                self.loss = 0.05 * self.value_loss + self.policy_loss - self.entropy * 0.05\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(ys=self.loss,xs=local_vars)\n",
    "                self.var_norms = tf.linalg.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,999.0)\n",
    "                \n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,a_size,trainer,model_path,global_episodes,make_gif=False):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.compat.v1.summary.FileWriter(model_path)\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        self.env = game\n",
    "        self.make_gif = make_gif\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        states = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        timesteps = rollout[:,3]\n",
    "        prev_rewards = [0] + rewards[:-1].tolist()\n",
    "        prev_actions = [0] + actions[:-1].tolist()\n",
    "        values = rollout[:,5]\n",
    "        \n",
    "        self.pr = prev_rewards\n",
    "        self.pa = prev_actions\n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        rnn_state = self.local_AC.state_init\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.state         : np.stack(states,axis=0),\n",
    "            self.local_AC.prev_rewards  : np.vstack(prev_rewards),\n",
    "            self.local_AC.prev_actions  : prev_actions,\n",
    "            self.local_AC.actions       : actions,\n",
    "            self.local_AC.timestep      : np.vstack(timesteps),\n",
    "            self.local_AC.advantages    : advantages,\n",
    "            self.local_AC.state_in[0]   : rnn_state[0],\n",
    "            self.local_AC.state_in[1]   : rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    def work(self,gamma,sess,coord,saver,train, num_episodes):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        \n",
    "        # set count to zero when loading a model\n",
    "        if not train:\n",
    "            episode_count = 0\n",
    "            \n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop() and episode_count <= num_episodes:\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                r = 0\n",
    "                a = 0\n",
    "                t = 0\n",
    "                s = self.env.reset()\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                \n",
    "                while d == False:\n",
    "                    #possible switch of S_2 <-> S_3 with probability 2.5% at the beginning of a trial (every two steps)\n",
    "                    if (self.env.state == S_1):\n",
    "                        self.env.possible_switch()\n",
    "                    \n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state_new = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={\n",
    "                        self.local_AC.state:[s],\n",
    "                        self.local_AC.prev_rewards:[[r]],\n",
    "                        self.local_AC.timestep:[[t]],\n",
    "                        self.local_AC.prev_actions:[a],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                    \n",
    "                    rnn_state = rnn_state_new\n",
    "                    s1,r,d,t = self.env.trial(a)\n",
    "                    episode_buffer.append([s,a,r,t,d,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "                    \n",
    "                    if episode_count % 100 == 0 and self.name == 'worker_0':\n",
    "                        if self.make_gif and self.env.last_state == S_2 or self.env.last_state == S_3:\n",
    "                            episode_frames.append(make_frame(frame_path,self.env.transitions,\n",
    "                                                                    self.env.get_rprobs(), \n",
    "                                                                    t, action=self.env.last_action, \n",
    "                                                                    final_state=self.env.last_state, \n",
    "                                                                    reward=r))\n",
    "                            \n",
    "                    \n",
    "                    \n",
    "                    episode_reward += r\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    s = s1\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the experience buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0 and train == True:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "            \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 10 == 0 and episode_count != 0:\n",
    "                    if episode_count % 100 == 0 and self.name == 'worker_0':\n",
    "                        if train == True:\n",
    "                            # save model\n",
    "                            os.makedirs(model_path+'/model-'+str(episode_count))\n",
    "                            saver.save(sess,model_path+'/model-'+str(episode_count)+\n",
    "                                       '/model-'+str(episode_count)+'.cptk')\n",
    "                            print (\"Saved Model\")\n",
    "                            \n",
    "                            # generate plot\n",
    "                            self.plot(episode_count,train)\n",
    "                            print (\"Saved Plot\")\n",
    "                        \n",
    "                        if self.make_gif and (not train):\n",
    "                            # generate gif\n",
    "                            make_gif(episode_frames,frame_path+\"/test_\"+str(episode_count)+'.gif')    \n",
    "                            print (\"Saved Gif\")\n",
    "                            \n",
    "                    # only track datapoints for training every 10 episoodes\n",
    "                    if train == True:    \n",
    "                        # For Tensorboard    \n",
    "                        mean_reward = np.mean(self.episode_rewards[-10:])\n",
    "                        mean_length = np.mean(self.episode_lengths[-10:])\n",
    "                        mean_value = np.mean(self.episode_mean_values[-10:])\n",
    "                        summary = tf.compat.v1.Summary()\n",
    "                        summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                        summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                        summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                        if train == True:\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                            summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                            summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                        self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                        self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "                if (episode_count % 10 == 0):\n",
    "                    print(\"episode_count is: \", episode_count)\n",
    "        if not train:            \n",
    "            self.plot(episode_count-1, train)\n",
    "\n",
    "    def plot(self, episode_count, train):\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(2)\n",
    "        ax.set_ylim([0.0, 1.0])\n",
    "        ax.set_ylabel('Stay Probability')\n",
    "        \n",
    "        stay_probs = self.env.stayProb()\n",
    "        \n",
    "        common = [stay_probs[0,0,0],stay_probs[1,0,0]]\n",
    "        uncommon = [stay_probs[0,1,0],stay_probs[1,1,0]]\n",
    "        \n",
    "        collect_seed_transition_probs.append([common,uncommon])\n",
    "        \n",
    "        ax.set_xticks([1.3,3.3])\n",
    "        ax.set_xticklabels(['Last trial rewarded', 'Last trial not rewarded'])\n",
    "        \n",
    "        c = plt.bar([1,3],  common, color='b', width=0.5)\n",
    "        uc = plt.bar([1.8,3.8], uncommon, color='r', width=0.5)\n",
    "        ax.legend( (c[0], uc[0]), ('common', 'uncommon') )\n",
    "        if train:\n",
    "            plt.savefig(plot_path +\"/\"+ 'train_' + str(episode_count) + \".png\")\n",
    "        else:\n",
    "            plt.savefig(plot_path +\"/\"+ 'test_' + str(episode_count) + \".png\")\n",
    "        self.env.transition_count = np.zeros((2,2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training/testing\n",
    "gamma = .9\n",
    "a_size = 2 \n",
    "n_seeds = 1\n",
    "num_episode_train = 20000\n",
    "num_episode_test = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_nb is: 0\n",
      "HIDDEN\n",
      "Tensor(\"global/concat:0\", shape=(None, 7), dtype=float32, device=/device:CPU:0)\n",
      "lstm_cell\n",
      "<keras.layers.rnn.legacy_cells.LSTMCell object at 0x7fe380309550>\n",
      "state_init\n",
      "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)]\n",
      "state_in\n",
      "(<tf.Tensor 'global/Placeholder_4:0' shape=(1, 48) dtype=float32>, <tf.Tensor 'global/Placeholder_5:0' shape=(1, 48) dtype=float32>)\n",
      "state_out\n",
      "(<tf.Tensor 'global/strided_slice_1:0' shape=(1, 48) dtype=float32>, <tf.Tensor 'global/strided_slice_2:0' shape=(1, 48) dtype=float32>)\n",
      "rnn_out\n",
      "Tensor(\"global/Reshape:0\", shape=(None, 48), dtype=float32, device=/device:CPU:0)\n",
      "HIDDEN\n",
      "Tensor(\"worker_0/concat:0\", shape=(None, 7), dtype=float32, device=/device:CPU:0)\n",
      "lstm_cell\n",
      "<keras.layers.rnn.legacy_cells.LSTMCell object at 0x7fe372222a90>\n",
      "state_init\n",
      "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)]\n",
      "state_in\n",
      "(<tf.Tensor 'worker_0/Placeholder_4:0' shape=(1, 48) dtype=float32>, <tf.Tensor 'worker_0/Placeholder_5:0' shape=(1, 48) dtype=float32>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marvin/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:332: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n",
      "/Users/Marvin/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_26659/639590902.py:16: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(48,state_is_tuple=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_out\n",
      "(<tf.Tensor 'worker_0/strided_slice_1:0' shape=(1, 48) dtype=float32>, <tf.Tensor 'worker_0/strided_slice_2:0' shape=(1, 48) dtype=float32>)\n",
      "rnn_out\n",
      "Tensor(\"worker_0/Reshape:0\", shape=(None, 48), dtype=float32, device=/device:CPU:0)\n",
      "Starting worker 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Marvin/opt/miniconda3/lib/python3.9/threading.py\", line 954, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/Marvin/opt/miniconda3/lib/python3.9/threading.py\", line 892, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_26659/2981986084.py\", line 58, in <lambda>\n",
      "  File \"/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_26659/1043959710.py\", line 129, in work\n",
      "  File \"/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_26659/1043959710.py\", line 21, in train\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (100, 6) + inhomogeneous part.\n"
     ]
    }
   ],
   "source": [
    "collect_seed_transition_probs = []\n",
    "\n",
    "# Do train and test for n_seeds different seeds\n",
    "for seed_nb in range(n_seeds):\n",
    "    \n",
    "    # initialize the directories' names to save the models for this particular seed\n",
    "    model_path = dir_name+'/model_' + str(seed_nb)\n",
    "    frame_path = dir_name+'/frames_' + str(seed_nb)\n",
    "    plot_path = dir_name+'/plots_' + str(seed_nb)\n",
    "    load_model_path = \"results/biorxiv/final/model_\" + str(seed_nb) + \"/model-20000\"\n",
    "    \n",
    "    # create the directories\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    if not os.path.exists(frame_path):\n",
    "        os.makedirs(frame_path)\n",
    "\n",
    "    if not os.path.exists(plot_path):\n",
    "        os.makedirs(plot_path)\n",
    "    \n",
    "    # in train don't load the model and set train=True\n",
    "    # in test, load the model and set train=False\n",
    "    for train, load_model, num_episodes in [[True,False,num_episode_train]]:\n",
    "        \n",
    "        print (\"seed_nb is:\", seed_nb)\n",
    "        \n",
    "        # resets tensorflow graph between train/test and seeds to avoid clutter\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"): \n",
    "            global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "            trainer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=7e-4)\n",
    "            master_network = AC_Network(a_size,'global',None) # Generate global network\n",
    "            num_workers = 1\n",
    "            workers = []\n",
    "            # Create worker classes\n",
    "            for i in range(num_workers):\n",
    "                workers.append(Worker(two_step_task(),i,a_size,trainer,model_path,global_episodes, make_gif=True))\n",
    "            saver = tf.compat.v1.train.Saver(max_to_keep=5)        \n",
    "        \n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            # set the seed\n",
    "            np.random.seed(seed_nb)\n",
    "            tf.compat.v1.set_random_seed(seed_nb)\n",
    "            \n",
    "            coord = tf.train.Coordinator()\n",
    "            if load_model == True:\n",
    "                print ('Loading Model...')\n",
    "                #ckpt = tf.train.get_checkpoint_state(load_model_path)\n",
    "                ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "            worker_threads = []\n",
    "            for worker in workers:\n",
    "                worker_work = lambda: worker.work(gamma,sess,coord,saver,train,num_episodes)\n",
    "                thread = threading.Thread(target=(worker_work))\n",
    "                thread.start()\n",
    "                worker_threads.append(thread)\n",
    "            coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_26659/1338533919.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_ylabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Stay Probability'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mcommon\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muncommon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcollect_seed_transition_probs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0mcommon_sum\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommon\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final plot of the different seeds\n",
    "\n",
    "episode_count = 300\n",
    "common_sum = np.array([0.,0.])\n",
    "uncommon_sum = np.array([0.,0.])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(n_seeds):\n",
    "\n",
    "    x = np.arange(2)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    ax.set_ylabel('Stay Probability')\n",
    "    \n",
    "    common, uncommon = collect_seed_transition_probs[i]\n",
    "    \n",
    "    common_sum += np.array(common)\n",
    "    uncommon_sum += np.array(uncommon)\n",
    "\n",
    "    ax.set_xticks([1.3,3.3])\n",
    "    ax.set_xticklabels(['Last trial rewarded', 'Last trial not rewarded'])\n",
    "\n",
    "    plt.plot([1,3], common, 'o', color='black');\n",
    "    plt.plot([1.8,3.8], uncommon, 'o', color='black');\n",
    "    \n",
    "c = plt.bar([1.,3.],  (1. / n_seeds) * common_sum, color='b', width=0.5)\n",
    "uc = plt.bar([1.8,3.8], (1. / n_seeds) * uncommon_sum, color='r', width=0.5)\n",
    "ax.legend( (c[0], uc[0]), ('common', 'uncommon') )\n",
    "plt.savefig(dir_name +\"/final_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b19d276b38a92b4edf272a17a0f3c1c5821b8b960b3f92721d58d5de9b921e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
