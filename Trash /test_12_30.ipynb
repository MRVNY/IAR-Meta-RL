{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marvin/opt/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.75  # Discount factor for past rewards\n",
    "max_steps_per_episode = 100 #10000\n",
    "# env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "# env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Step Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 11:17:16.606238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# encoding of the higher stages\n",
    "S_1 = 0\n",
    "S_2 = 1\n",
    "S_3 = 2\n",
    "nb_states = 3\n",
    "\n",
    "class two_step_task():\n",
    "    def __init__(self):\n",
    "        # start in S_1\n",
    "        self.state = S_1\n",
    "        \n",
    "        # defines what is the stage with the highest expected reward. Initially random\n",
    "        self.highest_reward_second_stage = np.random.choice([S_2,S_3])\n",
    "        \n",
    "        self.num_actions = 2\n",
    "        self.reset()\n",
    "        \n",
    "        # initialization of plotting variables\n",
    "        common_prob = 0.8\n",
    "        self.transitions = np.array([\n",
    "            [common_prob, 1-common_prob],\n",
    "            [1-common_prob, common_prob]\n",
    "        ])\n",
    "        self.transition_count = np.zeros((2,2,2))\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "    \n",
    "    def get_state(self):\n",
    "        one_hot_array = [0.,0.,0.]\n",
    "        one_hot_array[self.state] = 1.0\n",
    "        return tf.convert_to_tensor(one_hot_array)\n",
    "\n",
    "    def possible_switch(self):\n",
    "        if (np.random.uniform() < 0.025):\n",
    "            # switches which of S_2 or S_3 has expected reward of 0.9\n",
    "            self.highest_reward_second_stage = S_2 if (self.highest_reward_second_stage == S_3) else S_3\n",
    "            \n",
    "    def get_rprobs(self):\n",
    "        \"\"\"\n",
    "        probability of reward of states S_2 and S_3, in the form [[p, 1-p], [1-p, p]]\n",
    "        \"\"\"\n",
    "        if (self.highest_reward_second_stage == S_2):\n",
    "            r_prob = 0.9\n",
    "        else:\n",
    "            r_prob = 0.1\n",
    "        \n",
    "        rewards = np.array([\n",
    "            [r_prob, 1-r_prob],\n",
    "            [1-r_prob, r_prob]\n",
    "        ])\n",
    "        return rewards\n",
    "            \n",
    "    def isCommon(self,action,state):\n",
    "        if self.transitions[action][state] >= 1/2:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def updateStateProb(self,action):\n",
    "        if self.last_is_rewarded: #R\n",
    "            if self.last_is_common: #C\n",
    "                if self.last_action == action: #Rep\n",
    "                    self.transition_count[0,0,0] += 1\n",
    "                else: #URep\n",
    "                    self.transition_count[0,0,1] += 1\n",
    "            else: #UC\n",
    "                if self.last_action == action: #Rep\n",
    "                    self.transition_count[0,1,0] += 1\n",
    "                else: #URep\n",
    "                    self.transition_count[0,1,1] += 1\n",
    "        else: #UR\n",
    "            if self.last_is_common:\n",
    "                if self.last_action == action:\n",
    "                    self.transition_count[1,0,0] += 1\n",
    "                else:\n",
    "                    self.transition_count[1,0,1] += 1\n",
    "            else:\n",
    "                if self.last_action == action:\n",
    "                    self.transition_count[1,1,0] += 1\n",
    "                else:\n",
    "                    self.transition_count[1,1,1] += 1\n",
    "                    \n",
    "        \n",
    "    def stayProb(self):\n",
    "        print(self.transition_count)\n",
    "        row_sums = self.transition_count.sum(axis=-1)\n",
    "        stay_prob = self.transition_count / row_sums[:,:,np.newaxis] \n",
    "       \n",
    "        return stay_prob\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        \n",
    "        # for the two-step task plots\n",
    "        self.last_is_common = None\n",
    "        self.last_is_rewarded = None\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        \n",
    "        # come back to S_1 at the end of an episode\n",
    "        self.state = S_1\n",
    "        \n",
    "        return self.get_state()\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.timestep += 1\n",
    "        self.last_state = self.state\n",
    "        \n",
    "        # get next stage\n",
    "        if (self.state == S_1):\n",
    "            # get reward\n",
    "            reward = 0\n",
    "            # update stage\n",
    "            self.state = S_2 if (np.random.uniform() < self.transitions[action][0]) else S_3\n",
    "            # keep track of stay probability after first action\n",
    "            if (self.last_action != None):    \n",
    "                self.updateStateProb(action)\n",
    "            self.last_action = action\n",
    "            # book-keeping for plotting\n",
    "            self.last_is_common = self.isCommon(action,self.state-1)\n",
    "            \n",
    "        else:# case S_2 or S_3\n",
    "            # get probability of reward in stage\n",
    "            r_prob = 0.9 if (self.highest_reward_second_stage == self.state) else 0.1\n",
    "            # get reward\n",
    "            reward = 1 if np.random.uniform() < r_prob else 0\n",
    "            # update stage\n",
    "            self.state = S_1\n",
    "            # book-keeping for plotting\n",
    "            self.last_is_rewarded = reward\n",
    "\n",
    "        # new state after the decision\n",
    "        new_state = self.get_state()\n",
    "        if self.timestep >= 200: \n",
    "            done = True\n",
    "        else: \n",
    "            done = False\n",
    "        return new_state,reward,done,self.timestep\n",
    "    \n",
    "    def trial(self,action):\n",
    "        # do one action in S_1, and keep track of the perceptually distinguishable state you arive in\n",
    "        observation,_,_,_ = self.step(action)\n",
    "        # do the same action in the resulting state (S_2 or S_3). The action doesn't matter, the reward does\n",
    "        _,reward,done,_ = self.step(action)\n",
    "        return observation,reward,done,self.timestep\n",
    "    \n",
    "env = two_step_task()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n_step_return(\n",
    "    rewards: tf.Tensor,\n",
    "    values: tf.Tensor,\n",
    "    n: int,\n",
    "    gamma: float):\n",
    "    '''Fonction qui retourne R_t, le gamma utilisé est celui préconisé par \n",
    "    Wang et al. (2018), Methods/Simulation1\n",
    "    Version AVEC bootstrap (utilisation de la valeur prédite au dernier step\n",
    "    comme point de départ)\n",
    "    '''\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    #into the `returns` array\n",
    "    rewards = rewards[::-1]\n",
    "    values =  values[::-1]\n",
    "    \n",
    "    # values is inverted\n",
    "    discounted_sum = values[0]\n",
    "    for i in tf.range(n):\n",
    "        discounted_sum = rewards[i] + gamma * discounted_sum\n",
    "        returns = returns.write(i, int(discounted_sum))\n",
    "    \n",
    "    return returns.stack()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "        action_probs: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        rewards: tf.Tensor,\n",
    "        entropy : tf.Tensor,  \n",
    "        gamma: float = gamma,\n",
    "        beta_v: float = 0.05,\n",
    "        beta_e : float = 0.05 ) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "    \n",
    "    R_t = get_n_step_return(\n",
    "            rewards=rewards,\n",
    "            values=values, \n",
    "            n=rewards.shape[0], \n",
    "            gamma=gamma)\n",
    "    delta = tf.reshape(R_t,(100,1)) - values\n",
    "    delta_nogradient = tf.stop_gradient(delta)\n",
    "    # print(R_t)\n",
    "    # print(values)\n",
    "          \n",
    "    critic_loss = 0.5 * tf.reduce_sum(tf.square(delta))\n",
    "    action_log_probs = tf.math.log(action_probs + 1e-7)\n",
    "    actor_loss = tf.reduce_sum(action_log_probs * delta_nogradient)\n",
    "    total_loss = actor_loss + beta_v * critic_loss + beta_e * entropy\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 6 #states + reward + action = 3 + 1 + 2\n",
    "num_actions = 2\n",
    "num_hidden = 48\n",
    "\n",
    "inputs = layers.Input(shape=(None,num_inputs))\n",
    "common = layers.LSTM(num_hidden)(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 57.00 at episode 10\n",
      "reward: 44.00 at episode 20\n",
      "reward: 57.00 at episode 30\n",
      "reward: 54.00 at episode 40\n",
      "reward: 41.00 at episode 50\n",
      "reward: 38.00 at episode 60\n",
      "reward: 35.00 at episode 70\n",
      "reward: 36.00 at episode 80\n",
      "reward: 36.00 at episode 90\n",
      "reward: 34.00 at episode 100\n",
      "reward: 34.00 at episode 110\n",
      "reward: 27.00 at episode 120\n",
      "reward: 34.00 at episode 130\n",
      "reward: 33.00 at episode 140\n",
      "reward: 45.00 at episode 150\n",
      "reward: 37.00 at episode 160\n",
      "reward: 43.00 at episode 170\n",
      "reward: 38.00 at episode 180\n",
      "reward: 43.00 at episode 190\n",
      "reward: 36.00 at episode 200\n",
      "reward: 37.00 at episode 210\n",
      "reward: 32.00 at episode 220\n",
      "reward: 30.00 at episode 230\n",
      "reward: 37.00 at episode 240\n",
      "reward: 40.00 at episode 250\n",
      "reward: 31.00 at episode 260\n",
      "reward: 37.00 at episode 270\n",
      "reward: 36.00 at episode 280\n",
      "reward: 32.00 at episode 290\n",
      "reward: 46.00 at episode 300\n",
      "reward: 37.00 at episode 310\n",
      "reward: 24.00 at episode 320\n",
      "reward: 31.00 at episode 330\n",
      "reward: 48.00 at episode 340\n",
      "reward: 37.00 at episode 350\n",
      "reward: 37.00 at episode 360\n",
      "reward: 35.00 at episode 370\n",
      "reward: 29.00 at episode 380\n",
      "reward: 35.00 at episode 390\n",
      "reward: 37.00 at episode 400\n",
      "reward: 37.00 at episode 410\n",
      "reward: 32.00 at episode 420\n",
      "reward: 45.00 at episode 430\n",
      "reward: 34.00 at episode 440\n",
      "reward: 39.00 at episode 450\n",
      "reward: 38.00 at episode 460\n",
      "reward: 31.00 at episode 470\n",
      "reward: 37.00 at episode 480\n",
      "reward: 36.00 at episode 490\n",
      "reward: 29.00 at episode 500\n",
      "reward: 35.00 at episode 510\n",
      "reward: 41.00 at episode 520\n",
      "reward: 37.00 at episode 530\n",
      "reward: 33.00 at episode 540\n",
      "reward: 44.00 at episode 550\n",
      "reward: 33.00 at episode 560\n",
      "reward: 33.00 at episode 570\n",
      "reward: 41.00 at episode 580\n",
      "reward: 45.00 at episode 590\n",
      "reward: 33.00 at episode 600\n",
      "reward: 36.00 at episode 610\n",
      "reward: 37.00 at episode 620\n",
      "reward: 44.00 at episode 630\n",
      "reward: 42.00 at episode 640\n",
      "reward: 39.00 at episode 650\n",
      "reward: 42.00 at episode 660\n",
      "reward: 37.00 at episode 670\n",
      "reward: 35.00 at episode 680\n",
      "reward: 34.00 at episode 690\n",
      "reward: 31.00 at episode 700\n",
      "reward: 41.00 at episode 710\n",
      "reward: 42.00 at episode 720\n",
      "reward: 42.00 at episode 730\n",
      "reward: 35.00 at episode 740\n",
      "reward: 39.00 at episode 750\n",
      "reward: 39.00 at episode 760\n",
      "reward: 34.00 at episode 770\n",
      "reward: 42.00 at episode 780\n",
      "reward: 46.00 at episode 790\n",
      "reward: 45.00 at episode 800\n",
      "reward: 45.00 at episode 810\n",
      "reward: 36.00 at episode 820\n",
      "reward: 44.00 at episode 830\n",
      "reward: 39.00 at episode 840\n",
      "reward: 40.00 at episode 850\n",
      "reward: 45.00 at episode 860\n",
      "reward: 38.00 at episode 870\n",
      "reward: 48.00 at episode 880\n",
      "reward: 42.00 at episode 890\n",
      "reward: 33.00 at episode 900\n",
      "reward: 44.00 at episode 910\n",
      "reward: 42.00 at episode 920\n",
      "reward: 42.00 at episode 930\n",
      "reward: 42.00 at episode 940\n",
      "reward: 44.00 at episode 950\n",
      "reward: 29.00 at episode 960\n",
      "reward: 36.00 at episode 970\n",
      "reward: 38.00 at episode 980\n",
      "reward: 41.00 at episode 990\n",
      "reward: 34.00 at episode 1000\n",
      "reward: 37.00 at episode 1010\n",
      "reward: 42.00 at episode 1020\n",
      "reward: 39.00 at episode 1030\n",
      "reward: 40.00 at episode 1040\n",
      "reward: 43.00 at episode 1050\n",
      "reward: 38.00 at episode 1060\n",
      "reward: 48.00 at episode 1070\n",
      "reward: 32.00 at episode 1080\n",
      "reward: 40.00 at episode 1090\n",
      "reward: 48.00 at episode 1100\n",
      "reward: 34.00 at episode 1110\n",
      "reward: 35.00 at episode 1120\n",
      "reward: 43.00 at episode 1130\n",
      "reward: 34.00 at episode 1140\n",
      "reward: 37.00 at episode 1150\n",
      "reward: 43.00 at episode 1160\n",
      "reward: 39.00 at episode 1170\n",
      "reward: 39.00 at episode 1180\n",
      "reward: 39.00 at episode 1190\n",
      "reward: 48.00 at episode 1200\n",
      "reward: 32.00 at episode 1210\n",
      "reward: 48.00 at episode 1220\n",
      "reward: 37.00 at episode 1230\n",
      "reward: 48.00 at episode 1240\n",
      "reward: 39.00 at episode 1250\n",
      "reward: 46.00 at episode 1260\n",
      "reward: 39.00 at episode 1270\n",
      "reward: 43.00 at episode 1280\n",
      "reward: 34.00 at episode 1290\n",
      "reward: 46.00 at episode 1300\n",
      "reward: 41.00 at episode 1310\n",
      "reward: 44.00 at episode 1320\n",
      "reward: 37.00 at episode 1330\n",
      "reward: 41.00 at episode 1340\n",
      "reward: 46.00 at episode 1350\n",
      "reward: 44.00 at episode 1360\n",
      "reward: 42.00 at episode 1370\n",
      "reward: 40.00 at episode 1380\n",
      "reward: 31.00 at episode 1390\n",
      "reward: 52.00 at episode 1400\n",
      "reward: 46.00 at episode 1410\n",
      "reward: 45.00 at episode 1420\n",
      "reward: 42.00 at episode 1430\n",
      "reward: 47.00 at episode 1440\n",
      "reward: 44.00 at episode 1450\n",
      "reward: 44.00 at episode 1460\n",
      "reward: 48.00 at episode 1470\n",
      "reward: 34.00 at episode 1480\n",
      "reward: 41.00 at episode 1490\n",
      "reward: 42.00 at episode 1500\n",
      "reward: 46.00 at episode 1510\n",
      "reward: 42.00 at episode 1520\n",
      "reward: 43.00 at episode 1530\n",
      "reward: 41.00 at episode 1540\n",
      "reward: 43.00 at episode 1550\n",
      "reward: 41.00 at episode 1560\n",
      "reward: 50.00 at episode 1570\n",
      "reward: 42.00 at episode 1580\n",
      "reward: 54.00 at episode 1590\n",
      "reward: 50.00 at episode 1600\n",
      "reward: 53.00 at episode 1610\n",
      "reward: 52.00 at episode 1620\n",
      "reward: 46.00 at episode 1630\n",
      "reward: 42.00 at episode 1640\n",
      "reward: 34.00 at episode 1650\n",
      "reward: 46.00 at episode 1660\n",
      "reward: 45.00 at episode 1670\n",
      "reward: 43.00 at episode 1680\n",
      "reward: 46.00 at episode 1690\n",
      "reward: 38.00 at episode 1700\n",
      "reward: 49.00 at episode 1710\n",
      "reward: 40.00 at episode 1720\n",
      "reward: 48.00 at episode 1730\n",
      "reward: 38.00 at episode 1740\n",
      "reward: 52.00 at episode 1750\n",
      "reward: 41.00 at episode 1760\n",
      "reward: 44.00 at episode 1770\n",
      "reward: 56.00 at episode 1780\n",
      "reward: 56.00 at episode 1790\n",
      "reward: 48.00 at episode 1800\n",
      "reward: 43.00 at episode 1810\n",
      "reward: 43.00 at episode 1820\n",
      "reward: 37.00 at episode 1830\n",
      "reward: 45.00 at episode 1840\n",
      "reward: 41.00 at episode 1850\n",
      "reward: 43.00 at episode 1860\n",
      "reward: 40.00 at episode 1870\n",
      "reward: 55.00 at episode 1880\n",
      "reward: 48.00 at episode 1890\n",
      "reward: 50.00 at episode 1900\n",
      "reward: 44.00 at episode 1910\n",
      "reward: 50.00 at episode 1920\n",
      "reward: 50.00 at episode 1930\n",
      "reward: 33.00 at episode 1940\n",
      "reward: 43.00 at episode 1950\n",
      "reward: 41.00 at episode 1960\n",
      "reward: 42.00 at episode 1970\n",
      "reward: 46.00 at episode 1980\n",
      "reward: 56.00 at episode 1990\n",
      "reward: 47.00 at episode 2000\n",
      "reward: 43.00 at episode 2010\n",
      "reward: 49.00 at episode 2020\n",
      "reward: 48.00 at episode 2030\n",
      "reward: 52.00 at episode 2040\n",
      "reward: 52.00 at episode 2050\n",
      "reward: 52.00 at episode 2060\n",
      "reward: 44.00 at episode 2070\n",
      "reward: 44.00 at episode 2080\n",
      "reward: 46.00 at episode 2090\n",
      "reward: 46.00 at episode 2100\n",
      "reward: 42.00 at episode 2110\n",
      "reward: 51.00 at episode 2120\n",
      "reward: 55.00 at episode 2130\n",
      "reward: 37.00 at episode 2140\n",
      "reward: 44.00 at episode 2150\n",
      "reward: 41.00 at episode 2160\n",
      "reward: 41.00 at episode 2170\n",
      "reward: 45.00 at episode 2180\n",
      "reward: 49.00 at episode 2190\n",
      "reward: 51.00 at episode 2200\n",
      "reward: 43.00 at episode 2210\n",
      "reward: 54.00 at episode 2220\n",
      "reward: 44.00 at episode 2230\n",
      "reward: 55.00 at episode 2240\n",
      "reward: 45.00 at episode 2250\n",
      "reward: 52.00 at episode 2260\n",
      "reward: 47.00 at episode 2270\n",
      "reward: 45.00 at episode 2280\n",
      "reward: 43.00 at episode 2290\n",
      "reward: 50.00 at episode 2300\n",
      "reward: 48.00 at episode 2310\n",
      "reward: 38.00 at episode 2320\n",
      "reward: 47.00 at episode 2330\n",
      "reward: 41.00 at episode 2340\n",
      "reward: 56.00 at episode 2350\n",
      "reward: 43.00 at episode 2360\n",
      "reward: 44.00 at episode 2370\n",
      "reward: 49.00 at episode 2380\n",
      "reward: 43.00 at episode 2390\n",
      "reward: 45.00 at episode 2400\n",
      "reward: 43.00 at episode 2410\n",
      "reward: 43.00 at episode 2420\n",
      "reward: 55.00 at episode 2430\n",
      "reward: 48.00 at episode 2440\n",
      "reward: 44.00 at episode 2450\n",
      "reward: 41.00 at episode 2460\n",
      "reward: 43.00 at episode 2470\n",
      "reward: 45.00 at episode 2480\n",
      "reward: 53.00 at episode 2490\n",
      "reward: 47.00 at episode 2500\n",
      "reward: 49.00 at episode 2510\n",
      "reward: 54.00 at episode 2520\n",
      "reward: 43.00 at episode 2530\n",
      "reward: 42.00 at episode 2540\n",
      "reward: 45.00 at episode 2550\n",
      "reward: 51.00 at episode 2560\n",
      "reward: 53.00 at episode 2570\n",
      "reward: 48.00 at episode 2580\n",
      "reward: 61.00 at episode 2590\n",
      "reward: 36.00 at episode 2600\n",
      "reward: 54.00 at episode 2610\n",
      "reward: 53.00 at episode 2620\n",
      "reward: 44.00 at episode 2630\n",
      "reward: 49.00 at episode 2640\n",
      "reward: 47.00 at episode 2650\n",
      "reward: 55.00 at episode 2660\n",
      "reward: 50.00 at episode 2670\n",
      "reward: 45.00 at episode 2680\n",
      "reward: 54.00 at episode 2690\n",
      "reward: 46.00 at episode 2700\n",
      "reward: 52.00 at episode 2710\n",
      "reward: 50.00 at episode 2720\n",
      "reward: 47.00 at episode 2730\n",
      "reward: 53.00 at episode 2740\n",
      "reward: 51.00 at episode 2750\n",
      "reward: 42.00 at episode 2760\n",
      "reward: 53.00 at episode 2770\n",
      "reward: 41.00 at episode 2780\n",
      "reward: 48.00 at episode 2790\n",
      "reward: 46.00 at episode 2800\n",
      "reward: 56.00 at episode 2810\n",
      "reward: 53.00 at episode 2820\n",
      "reward: 50.00 at episode 2830\n",
      "reward: 46.00 at episode 2840\n",
      "reward: 46.00 at episode 2850\n",
      "reward: 49.00 at episode 2860\n",
      "reward: 42.00 at episode 2870\n",
      "reward: 55.00 at episode 2880\n",
      "reward: 56.00 at episode 2890\n",
      "reward: 51.00 at episode 2900\n",
      "reward: 45.00 at episode 2910\n",
      "reward: 42.00 at episode 2920\n",
      "reward: 42.00 at episode 2930\n",
      "reward: 51.00 at episode 2940\n",
      "reward: 51.00 at episode 2950\n",
      "reward: 50.00 at episode 2960\n",
      "reward: 52.00 at episode 2970\n",
      "reward: 45.00 at episode 2980\n",
      "reward: 49.00 at episode 2990\n",
      "reward: 50.00 at episode 3000\n",
      "reward: 50.00 at episode 3010\n",
      "reward: 48.00 at episode 3020\n",
      "reward: 54.00 at episode 3030\n",
      "reward: 44.00 at episode 3040\n",
      "reward: 46.00 at episode 3050\n",
      "reward: 57.00 at episode 3060\n",
      "reward: 55.00 at episode 3070\n",
      "reward: 50.00 at episode 3080\n",
      "reward: 52.00 at episode 3090\n",
      "reward: 44.00 at episode 3100\n",
      "reward: 42.00 at episode 3110\n",
      "reward: 48.00 at episode 3120\n",
      "reward: 55.00 at episode 3130\n",
      "reward: 43.00 at episode 3140\n",
      "reward: 47.00 at episode 3150\n",
      "reward: 49.00 at episode 3160\n",
      "reward: 46.00 at episode 3170\n",
      "reward: 51.00 at episode 3180\n",
      "reward: 56.00 at episode 3190\n",
      "reward: 48.00 at episode 3200\n",
      "reward: 53.00 at episode 3210\n",
      "reward: 47.00 at episode 3220\n",
      "reward: 45.00 at episode 3230\n",
      "reward: 41.00 at episode 3240\n",
      "reward: 37.00 at episode 3250\n",
      "reward: 47.00 at episode 3260\n",
      "reward: 54.00 at episode 3270\n",
      "reward: 46.00 at episode 3280\n",
      "reward: 40.00 at episode 3290\n",
      "reward: 53.00 at episode 3300\n",
      "reward: 48.00 at episode 3310\n",
      "reward: 50.00 at episode 3320\n",
      "reward: 52.00 at episode 3330\n",
      "reward: 58.00 at episode 3340\n",
      "reward: 51.00 at episode 3350\n",
      "reward: 47.00 at episode 3360\n",
      "reward: 42.00 at episode 3370\n",
      "reward: 45.00 at episode 3380\n",
      "reward: 46.00 at episode 3390\n",
      "reward: 55.00 at episode 3400\n",
      "reward: 50.00 at episode 3410\n",
      "reward: 46.00 at episode 3420\n",
      "reward: 47.00 at episode 3430\n",
      "reward: 50.00 at episode 3440\n",
      "reward: 49.00 at episode 3450\n",
      "reward: 48.00 at episode 3460\n",
      "reward: 47.00 at episode 3470\n",
      "reward: 55.00 at episode 3480\n",
      "reward: 51.00 at episode 3490\n",
      "reward: 51.00 at episode 3500\n",
      "reward: 52.00 at episode 3510\n",
      "reward: 54.00 at episode 3520\n",
      "reward: 53.00 at episode 3530\n",
      "reward: 48.00 at episode 3540\n",
      "reward: 45.00 at episode 3550\n",
      "reward: 49.00 at episode 3560\n",
      "reward: 43.00 at episode 3570\n",
      "reward: 49.00 at episode 3580\n",
      "reward: 43.00 at episode 3590\n",
      "reward: 48.00 at episode 3600\n",
      "reward: 44.00 at episode 3610\n",
      "reward: 38.00 at episode 3620\n",
      "reward: 44.00 at episode 3630\n",
      "reward: 56.00 at episode 3640\n",
      "reward: 41.00 at episode 3650\n",
      "reward: 54.00 at episode 3660\n",
      "reward: 50.00 at episode 3670\n",
      "reward: 45.00 at episode 3680\n",
      "reward: 54.00 at episode 3690\n",
      "reward: 47.00 at episode 3700\n",
      "reward: 57.00 at episode 3710\n",
      "reward: 47.00 at episode 3720\n",
      "reward: 46.00 at episode 3730\n",
      "reward: 53.00 at episode 3740\n",
      "reward: 42.00 at episode 3750\n",
      "reward: 56.00 at episode 3760\n",
      "reward: 43.00 at episode 3770\n",
      "reward: 48.00 at episode 3780\n",
      "reward: 56.00 at episode 3790\n",
      "reward: 55.00 at episode 3800\n",
      "reward: 47.00 at episode 3810\n",
      "reward: 55.00 at episode 3820\n",
      "reward: 52.00 at episode 3830\n",
      "reward: 46.00 at episode 3840\n",
      "reward: 49.00 at episode 3850\n",
      "reward: 45.00 at episode 3860\n",
      "reward: 45.00 at episode 3870\n",
      "reward: 53.00 at episode 3880\n",
      "reward: 59.00 at episode 3890\n",
      "reward: 44.00 at episode 3900\n",
      "reward: 47.00 at episode 3910\n",
      "reward: 44.00 at episode 3920\n",
      "reward: 41.00 at episode 3930\n",
      "reward: 59.00 at episode 3940\n",
      "reward: 48.00 at episode 3950\n",
      "reward: 55.00 at episode 3960\n",
      "reward: 48.00 at episode 3970\n",
      "reward: 54.00 at episode 3980\n",
      "reward: 52.00 at episode 3990\n",
      "reward: 49.00 at episode 4000\n",
      "reward: 49.00 at episode 4010\n",
      "reward: 58.00 at episode 4020\n",
      "reward: 56.00 at episode 4030\n",
      "reward: 51.00 at episode 4040\n",
      "reward: 45.00 at episode 4050\n",
      "reward: 49.00 at episode 4060\n",
      "reward: 43.00 at episode 4070\n",
      "reward: 45.00 at episode 4080\n",
      "reward: 47.00 at episode 4090\n",
      "reward: 43.00 at episode 4100\n",
      "reward: 47.00 at episode 4110\n",
      "reward: 61.00 at episode 4120\n",
      "reward: 53.00 at episode 4130\n",
      "reward: 48.00 at episode 4140\n",
      "reward: 51.00 at episode 4150\n",
      "reward: 48.00 at episode 4160\n",
      "reward: 39.00 at episode 4170\n",
      "reward: 54.00 at episode 4180\n",
      "reward: 47.00 at episode 4190\n",
      "reward: 49.00 at episode 4200\n",
      "reward: 39.00 at episode 4210\n",
      "reward: 43.00 at episode 4220\n",
      "reward: 51.00 at episode 4230\n",
      "reward: 45.00 at episode 4240\n",
      "reward: 53.00 at episode 4250\n",
      "reward: 46.00 at episode 4260\n",
      "reward: 35.00 at episode 4270\n",
      "reward: 49.00 at episode 4280\n",
      "reward: 47.00 at episode 4290\n",
      "reward: 48.00 at episode 4300\n",
      "reward: 44.00 at episode 4310\n",
      "reward: 41.00 at episode 4320\n",
      "reward: 43.00 at episode 4330\n",
      "reward: 49.00 at episode 4340\n",
      "reward: 56.00 at episode 4350\n",
      "reward: 53.00 at episode 4360\n",
      "reward: 46.00 at episode 4370\n",
      "reward: 44.00 at episode 4380\n",
      "reward: 49.00 at episode 4390\n",
      "reward: 44.00 at episode 4400\n",
      "reward: 52.00 at episode 4410\n",
      "reward: 45.00 at episode 4420\n",
      "reward: 50.00 at episode 4430\n",
      "reward: 51.00 at episode 4440\n",
      "reward: 49.00 at episode 4450\n",
      "reward: 48.00 at episode 4460\n",
      "reward: 49.00 at episode 4470\n",
      "reward: 48.00 at episode 4480\n",
      "reward: 45.00 at episode 4490\n",
      "reward: 57.00 at episode 4500\n",
      "reward: 55.00 at episode 4510\n",
      "reward: 53.00 at episode 4520\n",
      "reward: 49.00 at episode 4530\n",
      "reward: 48.00 at episode 4540\n",
      "reward: 51.00 at episode 4550\n",
      "reward: 41.00 at episode 4560\n",
      "reward: 57.00 at episode 4570\n",
      "reward: 46.00 at episode 4580\n",
      "reward: 48.00 at episode 4590\n",
      "reward: 43.00 at episode 4600\n",
      "reward: 52.00 at episode 4610\n",
      "reward: 51.00 at episode 4620\n",
      "reward: 58.00 at episode 4630\n",
      "reward: 51.00 at episode 4640\n",
      "reward: 47.00 at episode 4650\n",
      "reward: 52.00 at episode 4660\n",
      "reward: 39.00 at episode 4670\n",
      "reward: 44.00 at episode 4680\n",
      "reward: 55.00 at episode 4690\n",
      "reward: 43.00 at episode 4700\n",
      "reward: 44.00 at episode 4710\n",
      "reward: 49.00 at episode 4720\n",
      "reward: 50.00 at episode 4730\n",
      "reward: 53.00 at episode 4740\n",
      "reward: 58.00 at episode 4750\n",
      "reward: 54.00 at episode 4760\n",
      "reward: 46.00 at episode 4770\n",
      "reward: 58.00 at episode 4780\n",
      "reward: 43.00 at episode 4790\n",
      "reward: 54.00 at episode 4800\n",
      "reward: 52.00 at episode 4810\n",
      "reward: 54.00 at episode 4820\n",
      "reward: 61.00 at episode 4830\n",
      "reward: 43.00 at episode 4840\n",
      "reward: 52.00 at episode 4850\n",
      "reward: 45.00 at episode 4860\n",
      "reward: 52.00 at episode 4870\n",
      "reward: 41.00 at episode 4880\n",
      "reward: 48.00 at episode 4890\n",
      "reward: 53.00 at episode 4900\n",
      "reward: 48.00 at episode 4910\n",
      "reward: 51.00 at episode 4920\n",
      "reward: 54.00 at episode 4930\n",
      "reward: 43.00 at episode 4940\n",
      "reward: 50.00 at episode 4950\n",
      "reward: 48.00 at episode 4960\n",
      "reward: 49.00 at episode 4970\n",
      "reward: 46.00 at episode 4980\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_30326/1020041327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0m_ShapesFullySpecifiedAndEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       grad.dtype in (dtypes.int32, dtypes.float32)):\n\u001b[0;32m-> 1369\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" vs. \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6574\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6575\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6576\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6577\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[1;32m   6578\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "action_probs_history = tf.zeros((1,2))\n",
    "critic_value_history = tf.zeros((1,1))\n",
    "rewards_history = tf.zeros((1,1))\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "  \n",
    "\n",
    "for episode in range(1000):  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    reward = 0.0\n",
    "    action_probs = tf.zeros((2))\n",
    "    inputs = tf.zeros((1,1,6))\n",
    "    \n",
    "    episode_entropy = tf.zeros(())\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            \n",
    "            input = tf.concat([state, np.append(action_probs.numpy(),reward)],0)\n",
    "            input = tf.reshape(input, (1, 1, num_inputs))\n",
    "\n",
    "            inputs = tf.concat([inputs, input],1)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards from environment state\n",
    "            action_probs, critic_value = model(inputs)\n",
    "            \n",
    "            if np.isnan(action_probs.numpy()).any():\n",
    "                print(action_probs)\n",
    "                break                \n",
    "            \n",
    "            critic_value_history = tf.concat([critic_value_history, critic_value],0)\n",
    "\n",
    "            \n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history = tf.concat([action_probs_history,action_probs],0)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.trial(action)\n",
    "            #state, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "            rewards_history = tf.concat([rewards_history, tf.reshape(tf.convert_to_tensor([reward],dtype=tf.float32),(1,1))],0)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        #running_reward = episode_reward + running_reward\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        \n",
    "        #entropy\n",
    "        entropy = -tf.math.reduce_sum(tf.math.multiply(action_probs,tf.math.log(action_probs + 1e-7)))\n",
    "        episode_entropy += entropy\n",
    "\n",
    "        loss_value = compute_loss(\n",
    "            action_probs_history, \n",
    "            critic_value_history, \n",
    "            rewards_history, \n",
    "            tf.convert_to_tensor(episode_entropy))\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history = tf.zeros((1,2))\n",
    "        critic_value_history = tf.zeros((1,1))\n",
    "        rewards_history = tf.zeros((1,1))\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"reward: {:.2f} at episode {}\"\n",
    "        print(template.format(episode_reward, episode_count))\n",
    "        #print(env.stayProb())\n",
    "\n",
    "    if episode_reward > 90:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    \n",
    "    # if episode_count > 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "path = \"model_\" + datetime.now().strftime(\"%m%d-%H%M%S\") + \"_BAD_1000x100.h5\"\n",
    "model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b19d276b38a92b4edf272a17a0f3c1c5821b8b960b3f92721d58d5de9b921e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
