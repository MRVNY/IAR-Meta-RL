{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marvin/opt/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-01-05 15:17:55.518854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "from scipy.stats import entropy\n",
    "from scipy import signal\n",
    "import scipy as sp\n",
    "\n",
    "# Two Step Task\n",
    "from two_step_task import *\n",
    "env = two_step_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.9  # Discount factor for past rewards\n",
    "nb_steps = 100\n",
    "nb_episodes = 10000\n",
    "learning_rate = 7e-4\n",
    "bootstrap_n = 10\n",
    "\n",
    "beta_v = 0.05\n",
    "beta_e = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Paths\n",
    "path = \"train/\" + datetime.now().strftime(\"%m%d-%H:%M:%S\")\n",
    "log_dir = path+'/logs/'\n",
    "ckpt_dir = path+'/ckpt/'\n",
    "train_summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    return sp.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "        action_probs: tf.Tensor,\n",
    "        values: tf.Tensor,\n",
    "        rewards: tf.Tensor,\n",
    "        entropy : tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "    \n",
    "    bootstrap_n = tf.shape(rewards)[0]\n",
    "\n",
    "    # R_t = values\n",
    "    # R_t[-1] = 0\n",
    "    # for i in tf.range(bootstrap_n):\n",
    "    #     R_t[0:i] = rewards[0:i] + gamma * R_t[0:i]\n",
    "    \n",
    "    value_plus = np.asarray(values.numpy().tolist() + [bootstrap_n])\n",
    "    R_t = rewards + gamma * value_plus[1:] - value_plus[:-1]\n",
    "    R_t = discount(R_t,gamma)\n",
    "    \n",
    "    #R_t = tf.convert_to_tensor(R_t)\n",
    "    #R_t = tf.convert_to_tensor(R_t[::-1])\n",
    "    delta = R_t - values[::-1]\n",
    "\n",
    "    critic_loss = beta_v * 0.5 * tf.reduce_sum(tf.square(delta))\n",
    "    #critic_loss = beta_v * 0.5 * tf.reduce_sum(tf.square(delta * values))\n",
    "    #critic_loss = beta_v * tf.reduce_sum(delta * values)\n",
    "\n",
    "    \n",
    "    #actor_loss = tf.reduce_sum(tf.math.log(action_probs + 1e-7) * delta)\n",
    "    actor_loss = -tf.reduce_sum(tf.math.log(action_probs + 1e-7) * delta)\n",
    "    \n",
    "    entropy = beta_e * entropy\n",
    "\n",
    "    total_loss = actor_loss + critic_loss + entropy\n",
    "\n",
    "    return total_loss, actor_loss, critic_loss, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 7 #states + reward + action + timestep= 3 + 1 + 2 + 1\n",
    "num_actions = 2\n",
    "num_hidden = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(num_inputs))\n",
    "state_h = layers.Input(shape=(num_hidden))\n",
    "state_c = layers.Input(shape=(num_hidden))\n",
    "\n",
    "common, states = layers.LSTMCell(num_hidden)(inputs, states=[state_h, state_c], training=True)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=[inputs,state_h,state_c], outputs=[action, critic, states], )\n",
    "\n",
    "#model.save('init.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 59.00 at episode 0\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "running_reward = 0\n",
    "#model = keras.models.load_model('init.h5')\n",
    "  \n",
    "\n",
    "for episode in range(nb_episodes):  # Run until solved\n",
    "    state = env.reset()\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    episode_reward = 0\n",
    "    reward = 0.0\n",
    "    action_onehot = np.zeros((2))\n",
    "    cell_state = [tf.zeros((1,num_hidden)),tf.zeros((1,num_hidden))]\n",
    "    \n",
    "    episode_entropy = tf.zeros(())\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(nb_steps):\n",
    "            if (env.state == S_1):\n",
    "                env.possible_switch()\n",
    "                        \n",
    "            input = np.append(state.numpy(), action_onehot)\n",
    "            input = np.append(input,reward)\n",
    "            input = np.append(input,timestep)\n",
    "            input = tf.reshape(input, (1, num_inputs))\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards from environment state\n",
    "            action_probs, critic_value, cell_state = model([input,cell_state[0],cell_state[1]])\n",
    "            \n",
    "            critic_value_history.append(tf.squeeze(critic_value))\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action_probs = tf.squeeze(action_probs)\n",
    "            action = np.random.choice(num_actions, p=action_probs.numpy())\n",
    "            action_probs_history.append(action_probs[action])\n",
    "            action_onehot[action] = 1\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.trial(action)\n",
    "            #state, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            #entropy\n",
    "            #entropy = -tf.math.reduce_sum(tf.math.multiply(tmp,tf.math.log(tmp + 1e-7)))\n",
    "            entropy = sp.stats.entropy(action_probs)\n",
    "            episode_entropy += entropy\n",
    "            \n",
    "            if done: break\n",
    "\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        total_loss, actor_loss, critic_loss, entropy = compute_loss(\n",
    "            tf.convert_to_tensor(action_probs_history,dtype=tf.float32), \n",
    "            tf.convert_to_tensor(critic_value_history, dtype=tf.float32), \n",
    "            tf.convert_to_tensor(rewards_history, dtype=tf.float32), \n",
    "            episode_entropy)\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/total_loss', total_loss, step=episode)\n",
    "            tf.summary.scalar('loss/actor_loss', actor_loss, step=episode)\n",
    "            tf.summary.scalar('loss/critic_loss', critic_loss, step=episode)\n",
    "            tf.summary.scalar('loss/entropy', episode_entropy, step=episode)\n",
    "            tf.summary.scalar('game/reward', episode_reward, step=episode)\n",
    "            tf.summary.histogram('game/action_probs', action_probs_history, step=episode)\n",
    "\n",
    "    # Log details\n",
    "    if episode % 1000 == 0:\n",
    "        template = \"reward: {:.2f} at episode {}\"\n",
    "        print(template.format(episode_reward, episode))\n",
    "        checkpoint = tf.train.Checkpoint(model)\n",
    "        save_path = checkpoint.save(ckpt_dir+'checkpoints_'+str(episode)+'/two_steps.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(path+'/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"train/0104-16:37:22/model.h5\"\n",
    "# test_dir = \"test/\" + datetime.now().strftime(\"%m%d-%H:%M:%S\") +'/logs/'\n",
    "# test_summary_writer = tf.summary.create_file_writer(test_dir)\n",
    "\n",
    "# test_model = keras.models.load_model('/Users/Marvin/Documents/UNI/S9/IAR/two-step-task.nosync/train/0104-16:37:22/model.h5')\n",
    "# test_episode = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(test_episode):\n",
    "#     state = env.reset()\n",
    "#     action_probs_history = []\n",
    "#     critic_value_history = []\n",
    "#     rewards_history = []\n",
    "#     episode_reward = 0\n",
    "#     reward = 0.0\n",
    "#     action_onehot = np.zeros((2))\n",
    "#     cell_state = [tf.zeros((1,num_hidden)),tf.zeros((1,num_hidden))]\n",
    "    \n",
    "#     episode_entropy = tf.zeros(())\n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         for timestep in range(nb_steps):\n",
    "#             if (env.state == S_1):\n",
    "#                 env.possible_switch()\n",
    "            \n",
    "#             input = np.append(state.numpy(),action_onehot)\n",
    "#             input = np.append(input,reward)\n",
    "#             input = np.append(input,timestep)\n",
    "#             input = tf.reshape(input, (1, num_inputs))\n",
    "\n",
    "#             # Predict action probabilities and estimated future rewards from environment state\n",
    "#             action_probs, critic_value, cell_state = test_model([input,cell_state[0],cell_state[1]])\n",
    "            \n",
    "#             critic_value_history.append(tf.squeeze(critic_value))\n",
    "\n",
    "#             # Sample action from action probability distribution\n",
    "#             action_probs = tf.squeeze(action_probs)\n",
    "#             action = np.random.choice(num_actions, p=action_probs.numpy())\n",
    "#             action_probs_history.append(action_probs[action])\n",
    "#             action_onehot[action] = 1\n",
    "\n",
    "#             # Apply the sampled action in our environment\n",
    "#             state, reward, done, _ = env.trial(action)\n",
    "            \n",
    "#             print(env.get_rprobs())\n",
    "#             #state, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "#             rewards_history.append(reward)\n",
    "#             episode_reward += reward\n",
    "            \n",
    "#             #entropy\n",
    "#             #entropy = -tf.math.reduce_sum(tf.math.multiply(tmp,tf.math.log(tmp + 1e-7)))\n",
    "#             entropy = sp.stats.entropy(action_probs)\n",
    "#             episode_entropy += entropy\n",
    "            \n",
    "#             if done: break\n",
    "\n",
    "\n",
    "#         # Calculating loss values to update our network\n",
    "#         total_loss, actor_loss, critic_loss, entropy = compute_loss(\n",
    "#             tf.convert_to_tensor(action_probs_history,dtype=tf.float32), \n",
    "#             tf.convert_to_tensor(critic_value_history, dtype=tf.float32), \n",
    "#             tf.convert_to_tensor(rewards_history, dtype=tf.float32), \n",
    "#             episode_entropy)\n",
    "        \n",
    "#         with test_summary_writer.as_default():\n",
    "#             tf.summary.scalar('loss/total_loss', total_loss, step=episode)\n",
    "#             tf.summary.scalar('loss/actor_loss', actor_loss, step=episode)\n",
    "#             tf.summary.scalar('loss/critic_loss', critic_loss, step=episode)\n",
    "#             tf.summary.scalar('loss/entropy', episode_entropy, step=episode)\n",
    "#             tf.summary.scalar('game/reward', episode_reward, step=episode)\n",
    "#             tf.summary.histogram('game/action_probs', action_probs_history, step=episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env.stayProb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b19d276b38a92b4edf272a17a0f3c1c5821b8b960b3f92721d58d5de9b921e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
