{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marvin/opt/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "from scipy.stats import entropy\n",
    "from scipy import signal\n",
    "import scipy as sp\n",
    "\n",
    "# Two Step Task\n",
    "from two_step_task import *\n",
    "env = two_step_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.9  # Discount factor for past rewards\n",
    "nb_steps = 100\n",
    "nb_episodes = 20000\n",
    "learning_rate = 7e-4\n",
    "bootstrap_n = 10\n",
    "\n",
    "beta_v = 0.05\n",
    "beta_e = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 19:26:44.197681: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Save Paths\n",
    "path = \"train/\" + datetime.now().strftime(\"%m%d-%H:%M:%S\")\n",
    "log_dir = path+'/logs/'\n",
    "ckpt_dir = path+'/ckpt/'\n",
    "train_summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    return sp.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(action_probs, values, rewards, entropy):\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "    \n",
    "    bootstrap_n = tf.shape(rewards)[0]\n",
    "    \n",
    "    value_plus = np.append(values, bootstrap_n)\n",
    "    rewards_plus = np.append(rewards, bootstrap_n)\n",
    "    discounted_rewards = discount(rewards_plus,gamma)[:-1]\n",
    "    advantages = rewards + gamma * value_plus[1:] - value_plus[:-1]\n",
    "    advantages = discount(advantages,gamma)\n",
    "\n",
    "    critic_loss = beta_v * 0.5 * tf.reduce_sum(input_tensor=tf.square(discounted_rewards - tf.reshape(values,[-1])))\n",
    "    actor_loss = -tf.reduce_sum(tf.math.log(action_probs + 1e-7) * advantages)\n",
    "    entropy_loss = beta_e * entropy\n",
    "\n",
    "    total_loss = actor_loss + critic_loss + entropy\n",
    "\n",
    "    return total_loss, actor_loss, critic_loss, entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 7 #states + reward + action + timestep= 3 + 1 + 2 + 1\n",
    "num_actions = 2\n",
    "num_hidden = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(num_inputs))\n",
    "state_h = layers.Input(shape=(num_hidden))\n",
    "state_c = layers.Input(shape=(num_hidden))\n",
    "\n",
    "common, states = layers.LSTMCell(num_hidden)(inputs, states=[state_h, state_c], training=True)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=[inputs,state_h,state_c], outputs=[action, critic, states], )\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "#model.save('init.h5')\n",
    "#model = keras.models.load_model('init.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#start = datetime.now()\n",
    "for episode in range(nb_episodes):\n",
    "    with tf.GradientTape() as tape:\n",
    "        state = env.reset()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        reward = 0.0\n",
    "        action_onehot = np.zeros((2))\n",
    "        cell_state = [tf.zeros((1,num_hidden)),tf.zeros((1,num_hidden))]\n",
    "        entropy = 0.0\n",
    "    \n",
    "        for timestep in range(nb_steps):\n",
    "            input = np.concatenate((state, action_onehot, [reward], [timestep]),dtype = np.float32)\n",
    "            input = tf.expand_dims(input,0)\n",
    "            \n",
    "            # Predict action probabilities and estimated future rewards from environment state\n",
    "            action_probs, critic_value, cell_state = model([input,cell_state[0],cell_state[1]])\n",
    "            \n",
    "            critic_value_history.append(tf.squeeze(critic_value))\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action_probs = tf.squeeze(action_probs)\n",
    "            action = np.random.choice(num_actions, p=action_probs.numpy())\n",
    "            action_probs_history.append(action_probs[action])\n",
    "            action_onehot = np.zeros((2))\n",
    "            action_onehot[action] = 1.0\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.trial(action)\n",
    "            #state, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "            rewards_history.append(reward)\n",
    "            \n",
    "            # entropy\n",
    "            entropy += sp.stats.entropy(action_probs)\n",
    "        \n",
    "        # Calculating loss values to update our network\n",
    "        # total_loss, actor_loss, critic_loss, entropy_loss = compute_loss(\n",
    "        #     action_probs_history, critic_value_history, rewards_history, entropy)\n",
    "        \n",
    "        total_loss, actor_loss, critic_loss, entropy_loss = compute_loss(\n",
    "            tf.convert_to_tensor(action_probs_history,dtype=tf.float32), \n",
    "            tf.convert_to_tensor(critic_value_history, dtype=tf.float32), \n",
    "            tf.convert_to_tensor(rewards_history, dtype=tf.float32), \n",
    "            entropy)\n",
    "                \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # Log\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/total_loss', total_loss, step=episode)\n",
    "            tf.summary.scalar('loss/actor_loss', actor_loss, step=episode)\n",
    "            tf.summary.scalar('loss/critic_loss', critic_loss, step=episode)\n",
    "            tf.summary.scalar('loss/entropy', entropy_loss, step=episode)\n",
    "            tf.summary.scalar('game/reward', np.sum(rewards_history), step=episode)\n",
    "            tf.summary.histogram('game/action_probs', action_probs_history, step=episode)\n",
    "\n",
    "    # if episode % 10 == 0:\n",
    "    #     print(datetime.now() - start)\n",
    "        \n",
    "    # Checkpoint\n",
    "    if episode % 1000 == 0:\n",
    "        checkpoint = tf.train.Checkpoint(model)\n",
    "        save_path = checkpoint.save(ckpt_dir+'checkpoints_'+str(episode)+'/two_steps.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(path+'/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"train/0104-16:37:22/model.h5\"\n",
    "# test_dir = \"test/\" + datetime.now().strftime(\"%m%d-%H:%M:%S\") +'/logs/'\n",
    "# test_summary_writer = tf.summary.create_file_writer(test_dir)\n",
    "\n",
    "# test_model = keras.models.load_model('/Users/Marvin/Documents/UNI/S9/IAR/two-step-task.nosync/train/0104-16:37:22/model.h5')\n",
    "# test_episode = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(test_episode):\n",
    "#     state = env.reset()\n",
    "#     action_probs_history = []\n",
    "#     critic_value_history = []\n",
    "#     rewards_history = []\n",
    "#     episode_reward = 0\n",
    "#     reward = 0.0\n",
    "#     action_onehot = np.zeros((2))\n",
    "#     cell_state = [tf.zeros((1,num_hidden)),tf.zeros((1,num_hidden))]\n",
    "    \n",
    "#     episode_entropy = tf.zeros(())\n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         for timestep in range(nb_steps):\n",
    "#             if (env.state == S_1):\n",
    "#                 env.possible_switch()\n",
    "            \n",
    "#             input = np.append(state.numpy(),action_onehot)\n",
    "#             input = np.append(input,reward)\n",
    "#             input = np.append(input,timestep)\n",
    "#             input = tf.reshape(input, (1, num_inputs))\n",
    "\n",
    "#             # Predict action probabilities and estimated future rewards from environment state\n",
    "#             action_probs, critic_value, cell_state = test_model([input,cell_state[0],cell_state[1]])\n",
    "            \n",
    "#             critic_value_history.append(tf.squeeze(critic_value))\n",
    "\n",
    "#             # Sample action from action probability distribution\n",
    "#             action_probs = tf.squeeze(action_probs)\n",
    "#             action = np.random.choice(num_actions, p=action_probs.numpy())\n",
    "#             action_probs_history.append(action_probs[action])\n",
    "#             action_onehot[action] = 1\n",
    "\n",
    "#             # Apply the sampled action in our environment\n",
    "#             state, reward, done, _ = env.trial(action)\n",
    "            \n",
    "#             print(env.get_rprobs())\n",
    "#             #state, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "#             rewards_history.append(reward)\n",
    "#             episode_reward += reward\n",
    "            \n",
    "#             #entropy\n",
    "#             #entropy = -tf.math.reduce_sum(tf.math.multiply(tmp,tf.math.log(tmp + 1e-7)))\n",
    "#             entropy = sp.stats.entropy(action_probs)\n",
    "#             episode_entropy += entropy\n",
    "            \n",
    "#             if done: break\n",
    "\n",
    "\n",
    "#         # Calculating loss values to update our network\n",
    "#         total_loss, actor_loss, critic_loss, entropy = compute_loss(\n",
    "#             tf.convert_to_tensor(action_probs_history,dtype=tf.float32), \n",
    "#             tf.convert_to_tensor(critic_value_history, dtype=tf.float32), \n",
    "#             tf.convert_to_tensor(rewards_history, dtype=tf.float32), \n",
    "#             episode_entropy)\n",
    "        \n",
    "#         with test_summary_writer.as_default():\n",
    "#             tf.summary.scalar('loss/total_loss', total_loss, step=episode)\n",
    "#             tf.summary.scalar('loss/actor_loss', actor_loss, step=episode)\n",
    "#             tf.summary.scalar('loss/critic_loss', critic_loss, step=episode)\n",
    "#             tf.summary.scalar('loss/entropy', episode_entropy, step=episode)\n",
    "#             tf.summary.scalar('game/reward', episode_reward, step=episode)\n",
    "#             tf.summary.histogram('game/action_probs', action_probs_history, step=episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env.stayProb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b19d276b38a92b4edf272a17a0f3c1c5821b8b960b3f92721d58d5de9b921e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
