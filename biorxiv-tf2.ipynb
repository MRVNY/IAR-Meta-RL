{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-step-task as described in \"Prefrontal cortex as a meta-reinforcement learning system\"\n",
    "\n",
    "This iPython notebook includes an implementation of the two-step task as described here: [biorxiv link](https://www.biorxiv.org/content/early/2018/04/13/295964).\n",
    "\n",
    "The difference with our first trial (in `biorxiv-first-try.ipynb`) is that, in this notebook, **the A2C LSTM receives only actions made at stage 1 and the rewards received after leaving stage 2.** Therefore, the LSTM receives \"two-steps in one\", as implemented in \"trial\" from the class \"two_step_task()\".\n",
    "\n",
    "Note: in the biorxiv paper, they do not point out to any state in the first_stage, and refer to the second stage states as S_1 and S_2. In this notebook, we use the terminology presented in their [previous work on arxiv](https://arxiv.org/abs/1611.05763), where at the first stage there is one state S_1, and at the second stage there are two states S_2 and S_3.\n",
    "\n",
    "For this final step, the goal was to reproduce the plots from the [biorxiv pre-print](https://www.biorxiv.org/content/early/2018/04/13/295964) (Simulation 4, Figure b) ). To that end, we launched n=8 trainings using different seeds, but with the same hyperparameters as the paper, to compare to the results obtained by Wang et al.\n",
    "\n",
    "For each seed, the training consisted of 20k episodes of 100 trials (instead of 10k episodes of 100 trials in the paper). The reason for our number of episodes choice is that, in our case, the learning seemed to converge after around ~20k episodes for most seeds, without any significant gap in reward before ~15k episodes.\n",
    "\n",
    " ![reward curve](results/biorxiv/final/reward_curve.png)\n",
    " \n",
    "After training, we tested the 8 different models for 300 further episodes (like in the paper), with the weights of the LSTM being fixed. \n",
    "\n",
    "Here is the side by side comparison of our results (on the left) with the results from the paper (on the right):\n",
    "\n",
    "![side by side](results/biorxiv/final/side_by_side.png)\n",
    "\n",
    "Running the cells below will reproduce those tests. It will generate 8 different plots of the probabilities of repeating an action for a common/uncommon transition, if the last action was rewarded/unrewarded. Finally, it will average those plots to output a final plot, to reproduce the Figure b) from Simulation 4 in biorxiv. Each datapoint from a different seed is represented by a black dot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from helper import *\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_slim as slim\n",
    "import scipy.signal\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw \n",
    "from PIL import ImageFont\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories/book-keeping\n",
    "from datetime import datetime\n",
    "\n",
    "dir_name = \"train_\" + datetime.now().strftime(\"%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding of the higher stages\n",
    "S_1 = 0\n",
    "S_2 = 1\n",
    "S_3 = 2\n",
    "nb_states = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of our environment: the two-step task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_step_task():\n",
    "    def __init__(self):\n",
    "        # start in S_1\n",
    "        self.state = S_1\n",
    "        \n",
    "        # defines what is the stage with the highest expected reward. Initially random\n",
    "        self.highest_reward_second_stage = np.random.choice([S_2,S_3])\n",
    "        \n",
    "        self.num_actions = 2\n",
    "        self.reset()\n",
    "        \n",
    "        # initialization of plotting variables\n",
    "        common_prob = 0.8\n",
    "        self.transitions = np.array([\n",
    "            [common_prob, 1-common_prob],\n",
    "            [1-common_prob, common_prob]\n",
    "        ])\n",
    "        self.transition_count = np.zeros((2,2,2))\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "    \n",
    "    def get_state(self):\n",
    "        one_hot_array = np.zeros(nb_states)\n",
    "        one_hot_array[self.state] = 1\n",
    "        return one_hot_array\n",
    "\n",
    "    def possible_switch(self):\n",
    "        if (np.random.uniform() < 0.025):\n",
    "            # switches which of S_2 or S_3 has expected reward of 0.9\n",
    "            self.highest_reward_second_stage = S_2 if (self.highest_reward_second_stage == S_3) else S_3\n",
    "            \n",
    "    def get_rprobs(self):\n",
    "        \"\"\"\n",
    "        probability of reward of states S_2 and S_3, in the form [[p, 1-p], [1-p, p]]\n",
    "        \"\"\"\n",
    "        if (self.highest_reward_second_stage == S_2):\n",
    "            r_prob = 0.9\n",
    "        else:\n",
    "            r_prob = 0.1\n",
    "        \n",
    "        rewards = np.array([\n",
    "            [r_prob, 1-r_prob],\n",
    "            [1-r_prob, r_prob]\n",
    "        ])\n",
    "        return rewards\n",
    "            \n",
    "    def isCommon(self,action,state):\n",
    "        if self.transitions[action][state] >= 1/2:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def updateStateProb(self,action):\n",
    "        if self.last_is_rewarded: #R\n",
    "            if self.last_is_common: #C\n",
    "                if self.last_action == action: #Rep\n",
    "                    self.transition_count[0,0,0] += 1\n",
    "                else: #URep\n",
    "                    self.transition_count[0,0,1] += 1\n",
    "            else: #UC\n",
    "                if self.last_action == action: #Rep\n",
    "                    self.transition_count[0,1,0] += 1\n",
    "                else: #URep\n",
    "                    self.transition_count[0,1,1] += 1\n",
    "        else: #UR\n",
    "            if self.last_is_common:\n",
    "                if self.last_action == action:\n",
    "                    self.transition_count[1,0,0] += 1\n",
    "                else:\n",
    "                    self.transition_count[1,0,1] += 1\n",
    "            else:\n",
    "                if self.last_action == action:\n",
    "                    self.transition_count[1,1,0] += 1\n",
    "                else:\n",
    "                    self.transition_count[1,1,1] += 1\n",
    "                    \n",
    "        \n",
    "    def stayProb(self):\n",
    "        print(self.transition_count)\n",
    "        row_sums = self.transition_count.sum(axis=-1)\n",
    "        stay_prob = self.transition_count / row_sums[:,:,np.newaxis] \n",
    "       \n",
    "        return stay_prob\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        \n",
    "        # for the two-step task plots\n",
    "        self.last_is_common = None\n",
    "        self.last_is_rewarded = None\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        \n",
    "        # come back to S_1 at the end of an episode\n",
    "        self.state = S_1\n",
    "        \n",
    "        return self.get_state()\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.timestep += 1\n",
    "        self.last_state = self.state\n",
    "        \n",
    "        # get next stage\n",
    "        if (self.state == S_1):\n",
    "            # get reward\n",
    "            reward = 0\n",
    "            # update stage\n",
    "            self.state = S_2 if (np.random.uniform() < self.transitions[action][0]) else S_3\n",
    "            # keep track of stay probability after first action\n",
    "            if (self.last_action != None):    \n",
    "                self.updateStateProb(action)\n",
    "            self.last_action = action\n",
    "            # book-keeping for plotting\n",
    "            self.last_is_common = self.isCommon(action,self.state-1)\n",
    "            \n",
    "        else:# case S_2 or S_3\n",
    "            # get probability of reward in stage\n",
    "            r_prob = 0.9 if (self.highest_reward_second_stage == self.state) else 0.1\n",
    "            # get reward\n",
    "            reward = 1 if np.random.uniform() < r_prob else 0\n",
    "            # update stage\n",
    "            self.state = S_1\n",
    "            # book-keeping for plotting\n",
    "            self.last_is_rewarded = reward\n",
    "\n",
    "        # new state after the decision\n",
    "        new_state = self.get_state()\n",
    "        if self.timestep >= 200: \n",
    "            done = True\n",
    "        else: \n",
    "            done = False\n",
    "        return new_state,reward,done,self.timestep\n",
    "    \n",
    "    def trial(self,action):\n",
    "        # do one action in S_1, and keep track of the perceptually distinguishable state you arive in\n",
    "        observation,_,_,_ = self.step(action)\n",
    "        # do the same action in the resulting state (S_2 or S_3). The action doesn't matter, the reward does\n",
    "        _,reward,done,_ = self.step(action)\n",
    "        return observation,reward,done,self.timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,a_size,scope,trainer):\n",
    "        with tf.compat.v1.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.state = tf.keras.Input(shape=[None,3],dtype=tf.float32)\n",
    "            self.prev_rewards = tf.keras.Input(shape=[None,1],dtype=tf.float32)\n",
    "            self.prev_actions = tf.keras.Input(shape=[None],dtype=tf.int32)\n",
    "            self.timestep = tf.keras.Input(shape=[None,1],dtype=tf.float32)\n",
    "            self.prev_actions_onehot = tf.one_hot(self.prev_actions,a_size,dtype=tf.float32)\n",
    "\n",
    "            hidden = tf.concat([slim.flatten(self.state),self.prev_rewards,self.prev_actions_onehot,self.timestep],1)\n",
    "            \n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.keras.layers.LSTMCell(48,state_is_tuple=True)\n",
    "            \n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            \n",
    "            c_in = tf.keras.Input(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.keras.Input(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "            \n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(input=self.prev_rewards)[:1]\n",
    "            #state_in = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(c_in, h_in)\n",
    "            state_in = tf.keras.layers.LSTMStateTuple(c_in, h_in)\n",
    "\n",
    "            lstm_outputs, lstm_state = tf.compat.v1.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            #lstm_outputs, lstm_state = tf.keras.layers.LSTMCell(48,state_is_tuple=True)(rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                #time_major=False)\n",
    "            \n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 48])\n",
    "            \n",
    "            self.actions = tf.keras.Input(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                        \n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                weights_initializer=normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.target_v = tf.keras.Input(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.keras.Input(shape=[None],dtype=tf.float32)\n",
    "                \n",
    "                self.responsible_outputs = tf.reduce_sum(input_tensor=self.policy * self.actions_onehot, axis=[1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(input_tensor=tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(input_tensor=self.policy * tf.math.log(self.policy + 1e-7))\n",
    "                self.policy_loss = -tf.reduce_sum(input_tensor=tf.math.log(self.responsible_outputs + 1e-7)*self.advantages)\n",
    "                self.loss = 0.05 * self.value_loss + self.policy_loss - self.entropy * 0.05\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(ys=self.loss,xs=local_vars)\n",
    "                self.var_norms = tf.linalg.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,999.0)\n",
    "                \n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,a_size,trainer,model_path,global_episodes,make_gif=False):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.compat.v1.summary.FileWriter(model_path)\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        self.env = game\n",
    "        self.make_gif = make_gif\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        states = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        timesteps = rollout[:,3]\n",
    "        prev_rewards = [0] + rewards[:-1].tolist()\n",
    "        prev_actions = [0] + actions[:-1].tolist()\n",
    "        values = rollout[:,5]\n",
    "        \n",
    "        self.pr = prev_rewards\n",
    "        self.pa = prev_actions\n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        rnn_state = self.local_AC.state_init\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.state         : np.stack(states,axis=0),\n",
    "            self.local_AC.prev_rewards  : np.vstack(prev_rewards),\n",
    "            self.local_AC.prev_actions  : prev_actions,\n",
    "            self.local_AC.actions       : actions,\n",
    "            self.local_AC.timestep      : np.vstack(timesteps),\n",
    "            self.local_AC.advantages    : advantages,\n",
    "            self.local_AC.state_in[0]   : rnn_state[0],\n",
    "            self.local_AC.state_in[1]   : rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    def work(self,gamma,sess,coord,saver,train, num_episodes):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        \n",
    "        # set count to zero when loading a model\n",
    "        if not train:\n",
    "            episode_count = 0\n",
    "            \n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop() and episode_count <= num_episodes:\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                r = 0\n",
    "                a = 0\n",
    "                t = 0\n",
    "                s = self.env.reset()\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                \n",
    "                while d == False:\n",
    "                    #possible switch of S_2 <-> S_3 with probability 2.5% at the beginning of a trial (every two steps)\n",
    "                    if (self.env.state == S_1):\n",
    "                        self.env.possible_switch()\n",
    "                    \n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state_new = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={\n",
    "                        self.local_AC.state:[s],\n",
    "                        self.local_AC.prev_rewards:[[r]],\n",
    "                        self.local_AC.timestep:[[t]],\n",
    "                        self.local_AC.prev_actions:[a],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                    \n",
    "                    rnn_state = rnn_state_new\n",
    "                    s1,r,d,t = self.env.trial(a)\n",
    "                    episode_buffer.append([s,a,r,t,d,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "                    \n",
    "                    if episode_count % 100 == 0 and self.name == 'worker_0':\n",
    "                        if self.make_gif and self.env.last_state == S_2 or self.env.last_state == S_3:\n",
    "                            episode_frames.append(make_frame(frame_path,self.env.transitions,\n",
    "                                                                    self.env.get_rprobs(), \n",
    "                                                                    t, action=self.env.last_action, \n",
    "                                                                    final_state=self.env.last_state, \n",
    "                                                                    reward=r))\n",
    "                            \n",
    "                    \n",
    "                    \n",
    "                    episode_reward += r\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    s = s1\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the experience buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0 and train == True:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "            \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 10 == 0 and episode_count != 0:\n",
    "                    if episode_count % 100 == 0 and self.name == 'worker_0':\n",
    "                        if train == True:\n",
    "                            # save model\n",
    "                            os.makedirs(model_path+'/model-'+str(episode_count))\n",
    "                            saver.save(sess,model_path+'/model-'+str(episode_count)+\n",
    "                                       '/model-'+str(episode_count)+'.cptk')\n",
    "                            print (\"Saved Model\")\n",
    "                            \n",
    "                            # generate plot\n",
    "                            self.plot(episode_count,train)\n",
    "                            print (\"Saved Plot\")\n",
    "                        \n",
    "                        if self.make_gif and (not train):\n",
    "                            # generate gif\n",
    "                            make_gif(episode_frames,frame_path+\"/test_\"+str(episode_count)+'.gif')    \n",
    "                            print (\"Saved Gif\")\n",
    "                            \n",
    "                    # only track datapoints for training every 10 episoodes\n",
    "                    if train == True:    \n",
    "                        # For Tensorboard    \n",
    "                        mean_reward = np.mean(self.episode_rewards[-10:])\n",
    "                        mean_length = np.mean(self.episode_lengths[-10:])\n",
    "                        mean_value = np.mean(self.episode_mean_values[-10:])\n",
    "                        summary = tf.compat.v1.Summary()\n",
    "                        summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                        summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                        summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                        if train == True:\n",
    "                            summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                            summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                            summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                            summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                            summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                        self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                        self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "                if (episode_count % 10 == 0):\n",
    "                    print(\"episode_count is: \", episode_count)\n",
    "        if not train:            \n",
    "            self.plot(episode_count-1, train)\n",
    "\n",
    "    def plot(self, episode_count, train):\n",
    "        fig, ax = plt.subplots()\n",
    "        x = np.arange(2)\n",
    "        ax.set_ylim([0.0, 1.0])\n",
    "        ax.set_ylabel('Stay Probability')\n",
    "        \n",
    "        stay_probs = self.env.stayProb()\n",
    "        \n",
    "        common = [stay_probs[0,0,0],stay_probs[1,0,0]]\n",
    "        uncommon = [stay_probs[0,1,0],stay_probs[1,1,0]]\n",
    "        \n",
    "        collect_seed_transition_probs.append([common,uncommon])\n",
    "        \n",
    "        ax.set_xticks([1.3,3.3])\n",
    "        ax.set_xticklabels(['Last trial rewarded', 'Last trial not rewarded'])\n",
    "        \n",
    "        c = plt.bar([1,3],  common, color='b', width=0.5)\n",
    "        uc = plt.bar([1.8,3.8], uncommon, color='r', width=0.5)\n",
    "        ax.legend( (c[0], uc[0]), ('common', 'uncommon') )\n",
    "        if train:\n",
    "            plt.savefig(plot_path +\"/\"+ 'train_' + str(episode_count) + \".png\")\n",
    "        else:\n",
    "            plt.savefig(plot_path +\"/\"+ 'test_' + str(episode_count) + \".png\")\n",
    "        self.env.transition_count = np.zeros((2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder, # the encoder\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(layers.LSTM(64)), # making LSTM bidirectional\n",
    "    tf.keras.layers.Dense(32, activation='relu'), # FC layer for the classification part\n",
    "    tf.keras.layers.Dense(1) # final FC layer\n",
    "\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_a2c(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "\n",
    "    self.value_loss = 0.5 * tf.reduce_sum(input_tensor=tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "    self.entropy = - tf.reduce_sum(input_tensor=self.policy * tf.math.log(self.policy + 1e-7))\n",
    "    self.policy_loss = -tf.reduce_sum(input_tensor=tf.math.log(self.responsible_outputs + 1e-7)*self.advantages)\n",
    "    self.loss = 0.05 * self.value_loss + self.policy_loss - self.entropy * 0.05\n",
    "\n",
    "    #Get gradients from local network using local losses\n",
    "    local_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "    self.gradients = tf.gradients(ys=self.loss,xs=local_vars)\n",
    "    self.var_norms = tf.linalg.global_norm(local_vars)\n",
    "    grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,999.0)\n",
    "\n",
    "    #Apply local gradients to global network\n",
    "    global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "    self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))\n",
    "\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n",
    "\n",
    "#model.compile(optimizer='adam', loss=my_loss_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=loss_a2c,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training/testing\n",
    "gamma = .9\n",
    "a_size = 2 \n",
    "n_seeds = 8\n",
    "num_episode_train = 20000\n",
    "num_episode_test = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_nb is: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 3 for '{{node worker_0/concat}} = ConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32](worker_0/Flatten/flatten/Reshape, worker_0/input_2, worker_0/one_hot, worker_0/input_4, worker_0/concat/axis)' with input shapes: [?,?], [?,?,1], [?,?,2], [?,?,1], [].",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_14073/4220185946.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     37\u001B[0m             \u001B[0;31m# Create worker classes\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_workers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m                 \u001B[0mworkers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mWorker\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtwo_step_task\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0ma_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mglobal_episodes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmake_gif\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m             \u001B[0msaver\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSaver\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_to_keep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_14073/1043959710.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, game, name, a_size, trainer, model_path, global_episodes, make_gif)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0;31m#Create the local copy of the network and the tensorflow op to copy global paramters to local network\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlocal_AC\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAC_Network\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_local_ops\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mupdate_target_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'global'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_14073/2111474311.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, a_size, scope, trainer)\u001B[0m\n\u001B[1;32m      9\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprev_actions_onehot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mone_hot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprev_actions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0ma_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m             \u001B[0mhidden\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mslim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprev_rewards\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprev_actions_onehot\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimestep\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m             \u001B[0;31m#Recurrent network for temporal dependencies\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 153\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    154\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36m_create_c_op\u001B[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001B[0m\n\u001B[1;32m   1961\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInvalidArgumentError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1962\u001B[0m     \u001B[0;31m# Convert to ValueError for backwards compatibility.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1963\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1964\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1965\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mc_op\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Shape must be rank 2 but is rank 3 for '{{node worker_0/concat}} = ConcatV2[N=4, T=DT_FLOAT, Tidx=DT_INT32](worker_0/Flatten/flatten/Reshape, worker_0/input_2, worker_0/one_hot, worker_0/input_4, worker_0/concat/axis)' with input shapes: [?,?], [?,?,1], [?,?,2], [?,?,1], []."
     ]
    }
   ],
   "source": [
    "collect_seed_transition_probs = []\n",
    "\n",
    "# Do train and test for n_seeds different seeds\n",
    "for seed_nb in range(n_seeds):\n",
    "    \n",
    "    # initialize the directories' names to save the models for this particular seed\n",
    "    model_path = dir_name+'/model_' + str(seed_nb)\n",
    "    frame_path = dir_name+'/frames_' + str(seed_nb)\n",
    "    plot_path = dir_name+'/plots_' + str(seed_nb)\n",
    "    load_model_path = \"results/biorxiv/final/model_\" + str(seed_nb) + \"/model-20000\"\n",
    "    \n",
    "    # create the directories\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    if not os.path.exists(frame_path):\n",
    "        os.makedirs(frame_path)\n",
    "\n",
    "    if not os.path.exists(plot_path):\n",
    "        os.makedirs(plot_path)\n",
    "    \n",
    "    # in train don't load the model and set train=True\n",
    "    # in test, load the model and set train=False\n",
    "    for train, load_model, num_episodes in [[False, True, num_episode_test]]:#[[True,False,num_episode_train], [False, True, num_episode_test]]:\n",
    "        \n",
    "        print (\"seed_nb is:\", seed_nb)\n",
    "        \n",
    "        # resets tensorflow graph between train/test and seeds to avoid clutter\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"): \n",
    "            global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "            trainer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=7e-4)\n",
    "            master_network = AC_Network(a_size,'global',None) # Generate global network\n",
    "            num_workers = 1\n",
    "            workers = []\n",
    "            # Create worker classes\n",
    "            for i in range(num_workers):\n",
    "                workers.append(Worker(two_step_task(),i,a_size,trainer,model_path,global_episodes, make_gif=True))\n",
    "            saver = tf.compat.v1.train.Saver(max_to_keep=5)        \n",
    "        \n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            # set the seed\n",
    "            np.random.seed(seed_nb)\n",
    "            tf.compat.v1.set_random_seed(seed_nb)\n",
    "            \n",
    "            coord = tf.train.Coordinator()\n",
    "            if load_model == True:\n",
    "                print ('Loading Model...')\n",
    "                ckpt = tf.train.get_checkpoint_state(load_model_path)\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "            worker_threads = []\n",
    "            for worker in workers:\n",
    "                worker_work = lambda: worker.work(gamma,sess,coord,saver,train,num_episodes)\n",
    "                thread = threading.Thread(target=(worker_work))\n",
    "                thread.start()\n",
    "                worker_threads.append(thread)\n",
    "            coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final plot of the different seeds\n",
    "\n",
    "episode_count = 300\n",
    "common_sum = np.array([0.,0.])\n",
    "uncommon_sum = np.array([0.,0.])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(n_seeds):\n",
    "\n",
    "    x = np.arange(2)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    ax.set_ylabel('Stay Probability')\n",
    "    \n",
    "    common, uncommon = collect_seed_transition_probs[i]\n",
    "    \n",
    "    common_sum += np.array(common)\n",
    "    uncommon_sum += np.array(uncommon)\n",
    "\n",
    "    ax.set_xticks([1.3,3.3])\n",
    "    ax.set_xticklabels(['Last trial rewarded', 'Last trial not rewarded'])\n",
    "\n",
    "    plt.plot([1,3], common, 'o', color='black');\n",
    "    plt.plot([1.8,3.8], uncommon, 'o', color='black');\n",
    "    \n",
    "c = plt.bar([1.,3.],  (1. / n_seeds) * common_sum, color='b', width=0.5)\n",
    "uc = plt.bar([1.8,3.8], (1. / n_seeds) * uncommon_sum, color='r', width=0.5)\n",
    "ax.legend( (c[0], uc[0]), ('common', 'uncommon') )\n",
    "plt.savefig(dir_name +\"/final_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b19d276b38a92b4edf272a17a0f3c1c5821b8b960b3f92721d58d5de9b921e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
