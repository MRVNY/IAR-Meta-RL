{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marvin/opt/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'foraging_task'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yj/cc8p36j97cvgvkft3_74qy840000gp/T/ipykernel_94672/3310056272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Two Step Task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mforaging_task\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforaging_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'foraging_task'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "from scipy.stats import entropy\n",
    "from scipy import signal\n",
    "import scipy as sp\n",
    "\n",
    "# Two Step Task\n",
    "from two_armed_bandit import *\n",
    "env = foraging_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.75  # Discount factor for past rewards\n",
    "nb_steps = 100\n",
    "nb_episodes = 40000\n",
    "learning_rate = 7e-4\n",
    "bootstrap_n = 10\n",
    "\n",
    "beta_v = 0.05\n",
    "beta_e = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 14:57:23.397061: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Save Paths\n",
    "path = \"train_bandit/\" + datetime.now().strftime(\"%m%d-%H:%M:%S\")\n",
    "log_dir = path+'/logs/'\n",
    "ckpt_dir = path+'/ckpt/'\n",
    "train_summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    return sp.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(action_probs, values, rewards, entropy):\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "    \n",
    "    bootstrap_n = tf.shape(rewards)[0]\n",
    "    \n",
    "    value_plus = np.append(values, bootstrap_n)\n",
    "    rewards_plus = np.append(rewards, bootstrap_n)\n",
    "    discounted_rewards = discount(rewards_plus,gamma)[:-1]\n",
    "    advantages = rewards + gamma * value_plus[1:] - value_plus[:-1]\n",
    "    advantages = discount(advantages,gamma)\n",
    "\n",
    "    critic_loss = beta_v * 0.5 * tf.reduce_sum(input_tensor=tf.square(discounted_rewards - tf.reshape(values,[-1])))\n",
    "    actor_loss = -tf.reduce_sum(tf.math.log(action_probs + 1e-7) * advantages)\n",
    "    entropy_loss = beta_e * entropy\n",
    "\n",
    "    total_loss = actor_loss + critic_loss + entropy\n",
    "\n",
    "    return total_loss, actor_loss, critic_loss, entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4 #reward + action + timestep= 1 + 2 + 1\n",
    "num_actions = 2\n",
    "num_hidden = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(num_inputs))\n",
    "state_h = layers.Input(shape=(num_hidden))\n",
    "state_c = layers.Input(shape=(num_hidden))\n",
    "\n",
    "common, states = layers.LSTMCell(num_hidden)(inputs, states=[state_h, state_c], training=True)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=[inputs,state_h,state_c], outputs=[action, critic, states], )\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "#model.save('init.h5')\n",
    "#model = keras.models.load_model('init.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#start = datetime.now()\n",
    "for episode in range(nb_episodes):\n",
    "    with tf.GradientTape() as tape:\n",
    "        env.reset()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        reward = 0.0\n",
    "        action_onehot = np.zeros((2))\n",
    "        cell_state = [tf.zeros((1,num_hidden)),tf.zeros((1,num_hidden))]\n",
    "        entropy = 0.0\n",
    "    \n",
    "        for timestep in range(nb_steps):\n",
    "            input = np.concatenate((action_onehot, [reward], [timestep]),dtype = np.float32)\n",
    "            input = tf.expand_dims(input,0)\n",
    "            \n",
    "            # Predict action probabilities and estimated future rewards from environment state\n",
    "            action_probs, critic_value, cell_state = model([input,cell_state[0],cell_state[1]])\n",
    "            \n",
    "            critic_value_history.append(tf.squeeze(critic_value))\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action_probs = tf.squeeze(action_probs)\n",
    "            action = np.random.choice(num_actions, p=action_probs.numpy())\n",
    "            action_probs_history.append(action_probs[action])\n",
    "            action_onehot = np.zeros((2))\n",
    "            action_onehot[action] = 1.0\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            reward, done, _ = env.pullArm(action)\n",
    "            #state, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "            rewards_history.append(reward)\n",
    "            \n",
    "            # entropy\n",
    "            entropy += sp.stats.entropy(action_probs)\n",
    "            \n",
    "            if done: break\n",
    "        \n",
    "        # Calculating loss values to update our network\n",
    "        # total_loss, actor_loss, critic_loss, entropy_loss = compute_loss(\n",
    "        #     action_probs_history, critic_value_history, rewards_history, entropy)\n",
    "        \n",
    "        total_loss, actor_loss, critic_loss, entropy_loss = compute_loss(\n",
    "            tf.convert_to_tensor(action_probs_history,dtype=tf.float32), \n",
    "            tf.convert_to_tensor(critic_value_history, dtype=tf.float32), \n",
    "            tf.convert_to_tensor(rewards_history, dtype=tf.float32), \n",
    "            entropy)\n",
    "                \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # Log\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/total_loss', total_loss, step=episode)\n",
    "            tf.summary.scalar('loss/actor_loss', actor_loss, step=episode)\n",
    "            tf.summary.scalar('loss/critic_loss', critic_loss, step=episode)\n",
    "            tf.summary.scalar('loss/entropy', entropy_loss, step=episode)\n",
    "            tf.summary.scalar('game/reward', np.sum(rewards_history), step=episode)\n",
    "            tf.summary.histogram('game/action_probs', action_probs_history, step=episode)\n",
    "            \n",
    "\n",
    "    # if episode % 10 == 0:\n",
    "    #     print(datetime.now() - start)\n",
    "        \n",
    "    # Checkpoint\n",
    "    if episode % 2000 == 0:\n",
    "        checkpoint = tf.train.Checkpoint(model)\n",
    "        save_path = checkpoint.save(ckpt_dir+'checkpoints_'+str(episode)+'/two_steps.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(path+'/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model_path = \"train/0104-16:37:22/model.h5\"\n",
    "# test_dir = \"test/\" + datetime.now().strftime(\"%m%d-%H:%M:%S\") +'/logs/'\n",
    "# test_summary_writer = tf.summary.create_file_writer(test_dir)\n",
    "\n",
    "# test_model = keras.models.load_model('train/0105-21:52:39/model.h5')\n",
    "# #test_model = model\n",
    "# test_episode = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for episode in range(test_episode):\n",
    "#     state = env.reset()\n",
    "#     action_probs_history = []\n",
    "#     critic_value_history = []\n",
    "#     rewards_history = []\n",
    "#     episode_reward = 0\n",
    "#     reward = 0.0\n",
    "#     action_onehot = np.zeros((2))\n",
    "#     cell_state = [tf.zeros((1,num_hidden)),tf.zeros((1,num_hidden))]\n",
    "    \n",
    "#     episode_entropy = tf.zeros(())\n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         for timestep in range(nb_steps):\n",
    "            \n",
    "#             input = np.append(state,action_onehot)\n",
    "#             input = np.append(input,reward)\n",
    "#             input = np.append(input,timestep)\n",
    "#             input = tf.reshape(input, (1, num_inputs))\n",
    "\n",
    "#             # Predict action probabilities and estimated future rewards from environment state\n",
    "#             action_probs, critic_value, cell_state = test_model([input,cell_state[0],cell_state[1]])\n",
    "            \n",
    "#             critic_value_history.append(tf.squeeze(critic_value))\n",
    "\n",
    "#             # Sample action from action probability distribution\n",
    "#             action_probs = tf.squeeze(action_probs)\n",
    "#             action = np.random.choice(num_actions, p=action_probs.numpy())\n",
    "#             action_probs_history.append(action_probs[action])\n",
    "#             action_onehot[action] = 1\n",
    "\n",
    "#             # Apply the sampled action in our environment\n",
    "#             state, reward, done, _ = env.trial(action)\n",
    "            \n",
    "#             #state, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "#             rewards_history.append(reward)\n",
    "#             episode_reward += reward\n",
    "            \n",
    "#             #entropy\n",
    "#             #entropy = -tf.math.reduce_sum(tf.math.multiply(tmp,tf.math.log(tmp + 1e-7)))\n",
    "#             entropy = sp.stats.entropy(action_probs)\n",
    "#             episode_entropy += entropy\n",
    "            \n",
    "#             if done: break\n",
    "\n",
    "\n",
    "#         # # Calculating loss values to update our network\n",
    "#         # total_loss, actor_loss, critic_loss, entropy = compute_loss(\n",
    "#         #     tf.convert_to_tensor(action_probs_history,dtype=tf.float32), \n",
    "#         #     tf.convert_to_tensor(critic_value_history, dtype=tf.float32), \n",
    "#         #     tf.convert_to_tensor(rewards_history, dtype=tf.float32), \n",
    "#         #     episode_entropy)\n",
    "        \n",
    "#         # with test_summary_writer.as_default():\n",
    "#         #     tf.summary.scalar('loss/total_loss', total_loss, step=episode)\n",
    "#         #     tf.summary.scalar('loss/actor_loss', actor_loss, step=episode)\n",
    "#         #     tf.summary.scalar('loss/critic_loss', critic_loss, step=episode)\n",
    "#         #     tf.summary.scalar('loss/entropy', episode_entropy, step=episode)\n",
    "#         #     tf.summary.scalar('game/reward', episode_reward, step=episode)\n",
    "#         #     tf.summary.histogram('game/action_probs', action_probs_history, step=episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env.stayProb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# x = np.arange(2)\n",
    "# ax.set_ylim([0.0, 1.0])\n",
    "# ax.set_ylabel('Stay Probability')\n",
    "\n",
    "# stay_probs = env.stayProb()\n",
    "\n",
    "# common = [stay_probs[0,0,0],stay_probs[1,0,0]]\n",
    "# uncommon = [stay_probs[0,1,0],stay_probs[1,1,0]]\n",
    "\n",
    "# ax.set_xticks([1.3,3.3])\n",
    "# ax.set_xticklabels(['Last trial rewarded', 'Last trial not rewarded'])\n",
    "\n",
    "# c = plt.bar([1,3],  common, color='b', width=0.5)\n",
    "# uc = plt.bar([1.8,3.8], uncommon, color='r', width=0.5)\n",
    "# plt.ylim(0,1)\n",
    "# ax.legend( (c[0], uc[0]), ('common', 'uncommon') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b19d276b38a92b4edf272a17a0f3c1c5821b8b960b3f92721d58d5de9b921e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
