{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAR PROJET\n",
    "Hu Ruohui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from easydict import EasyDict as edict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoArmbandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.num_actions = 2\n",
    "        self.reset()\n",
    "    \n",
    "    #set the baseline probability of reward for action a.\n",
    "    #sampling from a uniform Benoulli distribution and held fix for the entire episode\n",
    "    def reset(self):\n",
    "        self.timestep = 0 \n",
    "        self.nb_al = 0\n",
    "        self.nb_ar = 0\n",
    "        self.nb = [0,0]\n",
    "        \n",
    "        #print(\"timestepmax\",self.timestepmax)\n",
    "        variance = np.random.uniform(0,.1)\n",
    "        self.baseline_prob = [variance,0.5-variance]\n",
    "        #print(\"baseline prob\",self.baseline_prob)\n",
    "        return self.state\n",
    "\n",
    "    ##get action from the network\n",
    "    def pullArm(self,action,prev_actions):\n",
    "        self.timestep += 1    \n",
    "        p_action_init = self.baseline_prob[action]\n",
    "        p_action = p_action_init   \n",
    "        #print(\"action\",action)\n",
    "        #print(\"prev_actions\",prev_actions)        \n",
    "        if action == 0 and len(prev_actions)!=0 :\n",
    "            if prev_actions[-1] == 0: \n",
    "                self.nb_al+=1\n",
    "                p_action = 1 - np.power((1-p_action),self.nb_al +1)\n",
    "                #print(\"nb_al\",self.nb_al)\n",
    "            else:\n",
    "                self.nb_al = 0 \n",
    "                #print(\"nb_al\",self.nb_al)\n",
    "            reward = random.choices([1,0],weights=[p_action,1-p_action])[0]\n",
    "            #print(\"reward_proba\",p_action,\"reward\",reward)\n",
    "        elif action == 1 and len(prev_actions)!=0: \n",
    "            if prev_actions[-1] == 1: \n",
    "                self.nb_ar+=1\n",
    "                p_action = 1 - np.power((1-p_action),self.nb_ar +1)\n",
    "                #print(\"nb_ar\",self.nb_ar)\n",
    "            else:\n",
    "                self.nb_ar = 0 \n",
    "                #print(\"nb_ar\",self.nb_ar)\n",
    "            reward = random.choices([1,0],weights=[p_action,1-p_action])[0]\n",
    "            #print(\"reward_proba\",p_action,\"reward\",reward)\n",
    "        else:\n",
    "            if action == 0 : \n",
    "                self.nb_al+=1\n",
    "                reward = random.choices([1,0],weights=[p_action,1-p_action])[0]\n",
    "                #print(\"reward_proba\",p_action,\"reward\",reward)\n",
    "            else :\n",
    "                self.nb_ar+=1\n",
    "                reward = random.choices([1,0],weights=[p_action,1-p_action])[0]\n",
    "                #print(\"reward_proba\",p_action,\"reward\",reward)\n",
    "        #print(\"timestep\",self.timestep,\"action\",action)\n",
    "        if self.timestep > self.timestepmax: \n",
    "            #print(\"nombre\",self.nb_al,self.nb_ar)\n",
    "            done = True\n",
    "        else: done = False\n",
    "        return  self.state,reward,done       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActorNet LSTM\n",
    "class ActorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,in_size,hidden_size,out_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_size, hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size,out_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x,2)  # log(softmax(x))\n",
    "        return x, hidden\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self,in_size,hidden_size,out_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_size, hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size,out_size)\n",
    "\n",
    "    def forward(self,x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoArmbandit()\n",
    "config = edict()\n",
    "config.Train = edict({\n",
    "    \"env_name\":\"TwoArmbandit\",\n",
    "    \"action_dim\":2,\n",
    "    \"bactch_size\":10,\n",
    "    \"lr\" : 0.001,\n",
    "    \"gamma\":0.75,\n",
    "    \"max_num_step\":random.randint(50,100),\n",
    "    \"num_episodes\":10,\n",
    "    \"hidden_size\":48,\n",
    "    \"seed\":88\n",
    "})\n",
    "\n",
    "def roll_out(actor_network,env,episode_len,value_network,init_state):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    is_done = False\n",
    "    final_reward = 0\n",
    "    state = init_state # 初始状态\n",
    "    a_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0) # 初始化隐状态\n",
    "    a_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0)\n",
    "    c_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0)\n",
    "    c_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    for j in range(config.Train.max_num_step):\n",
    "        states.append(state)\n",
    "        log_softmax_action, (a_hx,a_cx) = actor_network(Variable(T.Tensor([state]).unsqueeze(0)), (a_hx,a_cx))\n",
    "        # from torch.distributions import Categorical\n",
    "        softmax_action = T.exp(log_softmax_action) # 对数softmax取指数，保证大于0\n",
    "        action = np.random.choice(config.Train.action_dim,p=softmax_action.cpu().data.numpy()[0][0])\n",
    "    \n",
    "        one_hot_action = [int(k == action) for k in range(config.Train.action_dim)]\n",
    "        \n",
    "        reward,done,_ = env.pull(action)\n",
    "        next_state = np.delete(next_state, 1)\n",
    "        #fix_reward = -10 if done else 1\n",
    "        \n",
    "        actions.append(one_hot_action)\n",
    "        rewards.append(reward)\n",
    "        final_state = next_state \n",
    "        state = next_state\n",
    "        if done:\n",
    "            is_done = True\n",
    "            state = env.reset()\n",
    "            state = np.delete(state,1)\n",
    "            a_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "            a_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "            c_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "            c_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "            # 打印episode总分\n",
    "            print(j+1)\n",
    "            break\n",
    "    \n",
    "    return actions,rewards,final_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(r, gamma,final_r):\n",
    "    '''\n",
    "    r:          list\n",
    "    final_r:    scalar\n",
    "    '''\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = final_r\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # 初始化价值网络\n",
    "init_state = env.reset()\n",
    "value_network = ValueNetwork(in_size=4,hidden_size=config.Train.hidden_size, out_size=1)\n",
    "value_network_optim = T.optim.Adam(value_network.parameters(),lr=0.005)\n",
    "\n",
    "    # 初始化动作网络\n",
    "actor_network = ActorNetwork(in_size=4,hidden_size=config.Train.hidden_size, out_size=config.Train.action_dim)\n",
    "actor_network_optim = T.optim.Adam(actor_network.parameters(),lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb Cell 9\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_results \u001b[39m=\u001b[39m[]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mTrain\u001b[39m.\u001b[39mnum_episodes):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# 完成一轮rollout\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     states,actions,rewards,final_r,current_state \u001b[39m=\u001b[39m roll_out(actor_network,env,config\u001b[39m.\u001b[39;49mTrain\u001b[39m.\u001b[39;49mmax_num_step,value_network,init_state)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m#states.shape = [epi_len,3],list\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# rollout结束后的初态\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     init_state \u001b[39m=\u001b[39m current_state\n",
      "\u001b[1;32m/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb Cell 9\u001b[0m in \u001b[0;36mroll_out\u001b[0;34m(actor_network, env, episode_len, value_network, init_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mTrain\u001b[39m.\u001b[39mmax_num_step):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     states\u001b[39m.\u001b[39mappend(state)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     log_softmax_action, (a_hx,a_cx) \u001b[39m=\u001b[39m actor_network(Variable(T\u001b[39m.\u001b[39;49mTensor([state])\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)), (a_hx,a_cx))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# from torch.distributions import Categorical\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     softmax_action \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mexp(log_softmax_action) \u001b[39m# 对数softmax取指数，保证大于0\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb Cell 9\u001b[0m in \u001b[0;36mActorNetwork.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hidden):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     x, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(x,\u001b[39m2\u001b[39m)  \u001b[39m# log(softmax(x))\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/rnn.py:765\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[39mif\u001b[39;00m hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    763\u001b[0m             msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malso be 2-D but got (\u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D, \u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D) tensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 765\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    766\u001b[0m         hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m    768\u001b[0m \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "steps =[]\n",
    "task_episodes =[]\n",
    "test_results =[]\n",
    "\n",
    "for episode in range(config.Train.num_episodes):\n",
    "    # 完成一轮rollout\n",
    "    states,actions,rewards,final_r,current_state = roll_out(actor_network,env,config.Train.max_num_step,value_network,init_state)\n",
    "    #states.shape = [epi_len,3],list\n",
    "    \n",
    "    # rollout结束后的初态\n",
    "    init_state = current_state\n",
    "    actions_var = Variable(T.Tensor(actions).view(-1,config.Train.action_dim)).unsqueeze(0)\n",
    "    states_var = Variable(T.Tensor(states).view(-1,STATE_DIM=0)).unsqueeze(0)\n",
    "\n",
    "    # 训练动作网络\n",
    "    a_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    a_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    c_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    c_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    \n",
    "    actor_network_optim.zero_grad()\n",
    "    # print(states_var.unsqueeze(0).size())\n",
    "    log_softmax_actions, (a_hx,a_cx) = actor_network(states_var, (a_hx,a_cx))\n",
    "    vs, (c_hx,c_cx) = value_network(states_var, (c_hx,c_cx)) # 给出状态价值估计\n",
    "    vs.detach()    # 不参与求梯度\n",
    "    \n",
    "    # 计算Q(s,a)和Advantage函数\n",
    "    qs = Variable(T.Tensor(discount_reward(rewards,config.train.gamma,final_r)))\n",
    "    qs = qs.view(1, -1, 1)\n",
    "    advantages = qs - vs\n",
    "    # print('adv,',advantages.shape)\n",
    "    # log_softmax_actions * actions_var是利用独热编码特性取出对应action的对数概率\n",
    "    actor_network_loss = - T.mean(T.sum(log_softmax_actions*actions_var,1)* advantages)\n",
    "    actor_network_loss.backward()\n",
    "    T.nn.utils.clip_grad_norm(actor_network.parameters(),0.5)\n",
    "    actor_network_optim.step()\n",
    "\n",
    "    # 训练价值网络\n",
    "    value_network_optim.zero_grad()\n",
    "    target_values = qs\n",
    "    a_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    a_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    c_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    c_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "    values, (c_hx,c_cx) = value_network(states_var, (c_hx,c_cx))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    value_network_loss = criterion(values,target_values)\n",
    "    value_network_loss.backward()\n",
    "    T.nn.utils.clip_grad_norm(value_network.parameters(),0.5)\n",
    "    value_network_optim.step()\n",
    "\n",
    "    # Testing\n",
    "    if (episode + 1) % 50== 0:\n",
    "            result = 0\n",
    "            test_task = TwoArmbandit()\n",
    "            for test_epi in range(10):       # 测试10个episode\n",
    "                state = test_task.reset()\n",
    "                state = np.delete(state,1)\n",
    "                \n",
    "                a_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "                a_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "                c_hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "                c_cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0);\n",
    "                \n",
    "                for test_step in range(500): # 每个episode最长500frame\n",
    "                    \n",
    "                    log_softmax_actions, (a_hx,a_cx) = actor_network(Variable(T.Tensor([state]).view(1,1,3)), (a_hx,a_cx))\n",
    "                    softmax_action = T.exp(log_softmax_actions)\n",
    "                    \n",
    "                    #print(softmax_action.data)\n",
    "                    action = np.argmax(softmax_action.data.numpy()[0])\n",
    "                    next_state,reward,done,_ = test_task.step(action)\n",
    "                    next_state = np.delete(next_state,1)\n",
    "                    \n",
    "                    result += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(\"episode:\",episode+1,\"test result:\",result/10.0)\n",
    "            steps.append(episode+1)\n",
    "            test_results.append(result/10)\n",
    "plt.plot(steps,test_results)\n",
    "plt.savefig('training_score.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline proba tensor([[0.0648, 0.4352]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0507, 0.4493]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0528, 0.4472]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0896, 0.4104]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0700, 0.4300]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0714, 0.4286]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0717, 0.4283]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0223, 0.4777]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0175, 0.4825]])\n",
      "torch.Size([1, 1, 48])\n",
      "baseline proba tensor([[0.0457, 0.4543]])\n",
      "torch.Size([1, 1, 48])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "T.manual_seed(config.Train.seed)\n",
    "np.random.seed(config.Train.seed)\n",
    "\n",
    "dir = 'ckpt_' + config.Train.env_name\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "\n",
    "log_reward = []\n",
    "for i in range(config.Train.num_episodes):\n",
    "   \n",
    "    action_prob = T.Tensor([env.reset()])\n",
    "    print(\"baseline proba\",action_prob)\n",
    "    rewards = []\n",
    "    log_prob = []\n",
    "    hx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0)\n",
    "    cx = T.zeros(config.Train.hidden_size).unsqueeze(0).unsqueeze(0)\n",
    "    print(hx.shape)\n",
    "    #Model_ = A2C_LSTM(config,input_dim=4,num_actions=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3303.461669921875\n",
      "Loss: 2389.26806640625\n",
      "Loss: 797.8804931640625\n",
      "Loss: 1315.4296875\n",
      "Loss: 1078.1566162109375\n",
      "Loss: 1489.62451171875\n",
      "Loss: 702.7447509765625\n",
      "Loss: 1237.941162109375\n",
      "Loss: 461.96258544921875\n",
      "Loss: 840.1531982421875\n",
      "Loss: 1061.1978759765625\n",
      "Loss: 1644.6976318359375\n",
      "Loss: 413.14691162109375\n",
      "Loss: 240.18040466308594\n",
      "Loss: 428.4512939453125\n",
      "Loss: 881.3837280273438\n",
      "Loss: 669.3056640625\n",
      "Loss: 203.59217834472656\n",
      "Loss: 127.71015167236328\n",
      "Loss: 87.14710998535156\n",
      "Loss: 68.89632415771484\n",
      "Loss: 56.17485809326172\n",
      "Loss: 50.859161376953125\n",
      "Loss: 43.236778259277344\n",
      "Loss: 37.44258499145508\n",
      "Loss: 33.602603912353516\n",
      "Loss: 31.826881408691406\n",
      "Loss: 26.015975952148438\n",
      "Loss: 22.729454040527344\n",
      "Loss: 20.475494384765625\n",
      "Loss: 18.518329620361328\n",
      "Loss: 16.794355392456055\n",
      "Loss: 15.676040649414062\n",
      "Loss: 14.614242553710938\n",
      "Loss: 13.183671951293945\n",
      "Loss: 12.1857328414917\n",
      "Loss: 11.32094955444336\n",
      "Loss: 10.569906234741211\n",
      "Loss: 9.632092475891113\n",
      "Loss: 8.44007682800293\n",
      "Loss: 7.972111701965332\n",
      "Loss: 7.448755264282227\n",
      "Loss: 6.944090843200684\n",
      "Loss: 6.375226974487305\n",
      "Loss: 6.176793098449707\n",
      "Loss: 5.219420909881592\n",
      "Loss: 5.5407609939575195\n",
      "Loss: 4.324390411376953\n",
      "Loss: 4.086075782775879\n",
      "Loss: 3.8453006744384766\n",
      "Loss: 3.669084072113037\n",
      "Loss: 3.512077808380127\n",
      "Loss: 3.38592529296875\n",
      "Loss: 3.2948825359344482\n",
      "Loss: 3.2294578552246094\n",
      "Loss: 3.140087127685547\n",
      "Loss: 3.2483065128326416\n",
      "Loss: 2.8739969730377197\n",
      "Loss: 2.8105313777923584\n",
      "Loss: 2.4836316108703613\n",
      "Loss: 2.339510202407837\n",
      "Loss: 2.114849090576172\n",
      "Loss: 2.0073130130767822\n",
      "Loss: 1.8276208639144897\n",
      "Loss: 1.7887672185897827\n",
      "Loss: 1.6725342273712158\n",
      "Loss: 1.647745132446289\n",
      "Loss: 1.5663793087005615\n",
      "Loss: 1.5312057733535767\n",
      "Loss: 1.4511555433273315\n",
      "Loss: 1.443982481956482\n",
      "Loss: 1.3855403661727905\n",
      "Loss: 1.3624227046966553\n",
      "Loss: 1.3041918277740479\n",
      "Loss: 1.29409921169281\n",
      "Loss: 1.2463138103485107\n",
      "Loss: 1.2276347875595093\n",
      "Loss: 1.1802794933319092\n",
      "Loss: 1.168699860572815\n",
      "Loss: 1.1274492740631104\n",
      "Loss: 1.1117578744888306\n",
      "Loss: 1.0722689628601074\n",
      "Loss: 1.0586739778518677\n",
      "Loss: 1.0223774909973145\n",
      "Loss: 1.0077317953109741\n",
      "Loss: 0.9734411835670471\n",
      "Loss: 0.9589612483978271\n",
      "Loss: 0.9265564680099487\n",
      "Loss: 0.9116947054862976\n",
      "Loss: 0.8806307911872864\n",
      "Loss: 0.8651924729347229\n",
      "Loss: 0.8348491191864014\n",
      "Loss: 0.818350076675415\n",
      "Loss: 0.7880509495735168\n",
      "Loss: 0.7701743245124817\n",
      "Loss: 0.7398393154144287\n",
      "Loss: 0.721372127532959\n",
      "Loss: 0.6924570798873901\n",
      "Loss: 0.6755682826042175\n",
      "Loss: 0.6500796675682068\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "BATCH = 100;\n",
    "TIMESTEP = 100;\n",
    "HIDDEN = 40;\n",
    "EPOCHS = 100;\n",
    "\n",
    "class rnn_wrapper(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(rnn_wrapper, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=HIDDEN, hidden_size=HIDDEN, batch_first = True)  # 输入维度是3, 输出维度也是3\n",
    "        self.fc = nn.Linear(HIDDEN,1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#---------------train procedure---------------------------\n",
    "#---------------feed lstm by batch-------------------\n",
    "hidden_init = (torch.randn(1, BATCH, HIDDEN), torch.randn(1, BATCH, HIDDEN))\n",
    "hidden = hidden_init\n",
    "# print('Hidden:',hidden)\n",
    "\n",
    "rnn = rnn_wrapper()\n",
    "\n",
    "loss_function=nn.MSELoss()\n",
    "optimizer=optim.SGD(rnn.parameters(), lr=0.1)\n",
    "\n",
    "# target construct\n",
    "q = [(i*np.ones((BATCH,1))).tolist() for i in range(TIMESTEP)]\n",
    "target = torch.FloatTensor(q).transpose(0,1)\n",
    "\n",
    "# input construct\n",
    "k = np.random.randn(TIMESTEP,HIDDEN).tolist()\n",
    "inn = torch.FloatTensor([ k for i in range(BATCH)])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    rnn.zero_grad()\n",
    "    hidden = hidden_init\n",
    "\n",
    "    out, hidden = rnn(inn, hidden)\n",
    "\n",
    "    loss=loss_function(out, target)\n",
    "    print('Loss:',loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------test procedure---------------------------\n",
    "#---------------feed lstm per timestep-------------------\n",
    "hx,cx = hidden\n",
    "hx = hidden[0].select(1,0).unsqueeze(0);\n",
    "cx = hidden[1].select(1,0).unsqueeze(0);\n",
    "testin = inn.select(0,0);\n",
    "print(testin)\n",
    "\n",
    "for i in range(TIMESTEP):\n",
    "    out, (hx,cx) = rnn(testin.select(0,i).view(1,1,-1), (hx,cx))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = gym.make(\"CartPole-v0\")\n",
    "init_state = task.reset()\n",
    "init_state = np.delete(init_state,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04655312 -0.03280112 -0.03758502]\n"
     ]
    }
   ],
   "source": [
    "print(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    " \n",
    "NUM_PROCESSES = 16\n",
    " \n",
    "envs = [gym.make('Breakout-v0') for i in range(NUM_PROCESSES)]\n",
    " \n",
    "obs = [env.reset() for env in envs]\n",
    "obs = np.array(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(action_np)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action_np' is not defined"
     ]
    }
   ],
   "source": [
    "print(action_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 01:25:56.928980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicies\u001b[39;00m \u001b[39mimport\u001b[39;00m MlpPolicy\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvec_env\u001b[39;00m \u001b[39mimport\u001b[39;00m SubprocVecEnv\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ruohuihu/Downloads/Meta-RL-master/Version_hu.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m \u001b[39mimport\u001b[39;00m A2C\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/stable_baselines/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39macer\u001b[39;00m \u001b[39mimport\u001b[39;00m ACER\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39macktr\u001b[39;00m \u001b[39mimport\u001b[39;00m ACKTR\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m \u001b[39mimport\u001b[39;00m DQN\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mher\u001b[39;00m \u001b[39mimport\u001b[39;00m HER\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mppo2\u001b[39;00m \u001b[39mimport\u001b[39;00m PPO2\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/stable_baselines/deepq/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicies\u001b[39;00m \u001b[39mimport\u001b[39;00m MlpPolicy, CnnPolicy, LnMlpPolicy, LnCnnPolicy\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuild_graph\u001b[39;00m \u001b[39mimport\u001b[39;00m build_act, build_train  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdqn\u001b[39;00m \u001b[39mimport\u001b[39;00m DQN\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/test/lib/python3.8/site-packages/stable_baselines/deepq/policies.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf_layers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspaces\u001b[39;00m \u001b[39mimport\u001b[39;00m Discrete\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines import A2C\n",
    "\n",
    "# multiprocess environment\n",
    "n_cpu = 4\n",
    "env = SubprocVecEnv([lambda: gym.make('CartPole-v1') for i in range(n_cpu)])\n",
    "\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=20)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable_baselines\n",
      "  Downloading stable_baselines-2.10.2-py3-none-any.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=0.5.5 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (2.0.0)\n",
      "Requirement already satisfied: pandas in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (1.4.1)\n",
      "Requirement already satisfied: scipy in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (1.7.1)\n",
      "Requirement already satisfied: matplotlib in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (3.4.2)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (0.21.0)\n",
      "Requirement already satisfied: joblib in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (0.17.0)\n",
      "Requirement already satisfied: numpy in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (1.22.2)\n",
      "Requirement already satisfied: opencv-python in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from stable_baselines) (4.5.5.62)\n",
      "Requirement already satisfied: ale-py~=0.7.1 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from gym[atari,classic_control]>=0.11->stable_baselines) (0.7.4)\n",
      "Collecting pyglet>=1.4.0\n",
      "  Downloading pyglet-2.0.3-py3-none-any.whl (968 kB)\n",
      "\u001b[K     |████████████████████████████████| 968 kB 54.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from ale-py~=0.7.1->gym[atari,classic_control]>=0.11->stable_baselines) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from ale-py~=0.7.1->gym[atari,classic_control]>=0.11->stable_baselines) (4.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.1->gym[atari,classic_control]>=0.11->stable_baselines) (3.7.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from matplotlib->stable_baselines) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from matplotlib->stable_baselines) (9.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from matplotlib->stable_baselines) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from matplotlib->stable_baselines) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from matplotlib->stable_baselines) (1.3.1)\n",
      "Requirement already satisfied: six in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->stable_baselines) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ruohuihu/opt/anaconda3/envs/test/lib/python3.8/site-packages (from pandas->stable_baselines) (2021.3)\n",
      "Installing collected packages: pyglet, stable-baselines\n",
      "Successfully installed pyglet-2.0.3 stable-baselines-2.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stable_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (default, Jan  8 2020, 16:15:59) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a7478e9944679ce006cd3e4973d295a5c67f46e807b688e0676d68ebc341d27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
